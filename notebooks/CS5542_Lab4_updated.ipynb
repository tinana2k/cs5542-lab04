{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinana2k/cs5542-lab04/blob/main/notebooks/CS5542_Lab4_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3e229a",
      "metadata": {
        "id": "df3e229a"
      },
      "source": [
        "# CS 5542 ‚Äî Lab 4 Notebook (Team Project)\n",
        "## RAG Application Integration, Deployment, and Monitoring (Deadline: Feb. 12, 2026)\n",
        "\n",
        "**Purpose:** This notebook is a **project-aligned template** for Lab 4. Your team should reuse your Lab-3 multimodal RAG pipeline and integrate it into a **deployable application** with **automatic logging** and **failure analysis**.\n",
        "\n",
        "### Submission policy\n",
        "- **Survey:** submitted **individually**\n",
        "- **Deliverables (GitHub repo / notebook / report / deployment link):** submitted **as a team**\n",
        "\n",
        "### Team-size requirement\n",
        "- **1‚Äì2 students:** Base requirements + **1 extension**\n",
        "- **3‚Äì4 students:** Base requirements + **2‚Äì3 extensions**\n",
        "\n",
        "---\n",
        "\n",
        "## What you will build (at minimum)\n",
        "1. A **Streamlit app** that accepts a question and returns:\n",
        "   - an **answer**\n",
        "   - **retrieved evidence** with citations\n",
        "   - **metrics panel** (latency, P@5, R@10 if applicable)\n",
        "2. An **automatic logger** that appends to: `logs/query_metrics.csv`\n",
        "3. A **mini gold set** of **5 project queries** (Q1‚ÄìQ5) for evaluation\n",
        "4. **Two failure cases** with root cause + proposed fix\n",
        "\n",
        "> **Important:** Lab 4 focuses on **application integration and deployment**, not on redesigning retrieval. Prefer reusing your Lab-3 modules.\n",
        "\n",
        "---\n",
        "\n",
        "## Recommended repository structure (for your team repo)\n",
        "```\n",
        "/app/              # Streamlit UI (required)\n",
        "/rag/              # Retrieval + indexing modules (reuse from Lab 3)\n",
        "/logs/             # query_metrics.csv (auto-created)\n",
        "/data/             # your project-aligned PDFs/images (do NOT commit large/private data)\n",
        "/api/              # optional FastAPI backend (extension)\n",
        "/notebooks/        # this notebook\n",
        "requirements.txt\n",
        "README.md\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Contents of this notebook\n",
        "1. Setup & environment checks  \n",
        "2. Project dataset wiring (connect your Lab-3 ingestion)  \n",
        "3. Mini gold set (Q1‚ÄìQ5)  \n",
        "4. Retrieval + answer function (reuse your Lab-3 pipeline)  \n",
        "5. Evaluation + logging (required)  \n",
        "6. Streamlit app skeleton (required)  \n",
        "7. Optional extension: FastAPI backend  \n",
        "8. Deployment checklist + failure analysis template\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "xYMU614tBEP3"
      },
      "id": "xYMU614tBEP3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os, zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "zip_path = \"data.zip\"\n",
        "\n",
        "# Remove old extracted folder if it exists (prevents stale files)\n",
        "if Path(\"data\").exists():\n",
        "    shutil.rmtree(\"data\")\n",
        "\n",
        "# Extract fresh\n",
        "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "    z.extractall(\".\")\n",
        "\n",
        "print(\"‚úÖ Extracted fresh\")\n",
        "print(\"Docs now:\", sorted(os.listdir(\"data/docs\")) if Path(\"data/docs\").exists() else \"NO data/docs folder\")\n",
        "print(\"Images now:\", sorted(os.listdir(\"data/images\")) if Path(\"data/images\").exists() else \"NO data/images folder\")\n"
      ],
      "metadata": {
        "id": "nX_ts48FGJs3"
      },
      "id": "nX_ts48FGJs3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jdz6kAJaThoY",
      "metadata": {
        "id": "Jdz6kAJaThoY"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "docs_dir = Path(\"data/docs\")\n",
        "docs_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Only create numeric demo if explicitly needed\n",
        "create_demo_numeric = False   # üî¥ Change to True only if required by lab rubric\n",
        "\n",
        "if create_demo_numeric:\n",
        "    numeric_path = docs_dir / \"numeric_demo.txt\"\n",
        "    numeric_path.write_text(\n",
        "        \"Fusion Hyperparameters (Table 1)\\n\"\n",
        "        \"alpha = 0.50\\n\"\n",
        "        \"top_k = 5\\n\"\n",
        "        \"missing_evidence_score_threshold = 0.05\\n\"\n",
        "        \"latency_alert_ms = 2000\\n\",\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    print(\"‚úÖ Created numeric demo file\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Skipping numeric demo file creation (not in dataset)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8AOvgEV9Q-8g",
      "metadata": {
        "id": "8AOvgEV9Q-8g"
      },
      "outputs": [],
      "source": [
        "# Sanity check: ensure PDF docs are loaded\n",
        "import os, glob\n",
        "\n",
        "doc_files = glob.glob('./data/docs/*.pdf')\n",
        "print(\"Found PDF docs:\", len(doc_files))\n",
        "\n",
        "assert len(doc_files) > 0, \"No PDF docs found. Ensure the ZIP was extracted correctly.\"\n",
        "\n",
        "# Preview first PDF filename (we can't directly print PDF text yet)\n",
        "print(\"First document:\", os.path.basename(doc_files[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install PyPDF2\n"
      ],
      "metadata": {
        "id": "ehVY1_TmIjsp"
      },
      "id": "ehVY1_TmIjsp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pycryptodome"
      ],
      "metadata": {
        "id": "79SF43rYJtg9"
      },
      "id": "79SF43rYJtg9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pycryptodome cryptography\n"
      ],
      "metadata": {
        "id": "7KKGF8yWKa2c"
      },
      "id": "7KKGF8yWKa2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Crypto\n",
        "print(\"Crypto OK:\", Crypto.__version__)\n"
      ],
      "metadata": {
        "id": "-BwsmnpnKiXM"
      },
      "id": "-BwsmnpnKiXM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xGbb-SjVSN21",
      "metadata": {
        "id": "xGbb-SjVSN21"
      },
      "outputs": [],
      "source": [
        "import glob, os, sys, subprocess\n",
        "\n",
        "# Ensure dependencies\n",
        "def pip_install(pkg):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "try:\n",
        "    from PyPDF2 import PdfReader\n",
        "except ModuleNotFoundError:\n",
        "    pip_install(\"PyPDF2\")\n",
        "    from PyPDF2 import PdfReader\n",
        "\n",
        "# Needed for AES-encrypted PDFs\n",
        "try:\n",
        "    import Crypto  # noqa: F401\n",
        "except ModuleNotFoundError:\n",
        "    pip_install(\"pycryptodome\")\n",
        "\n",
        "DOC_DIR = \"./data/docs\"\n",
        "doc_files = sorted(glob.glob(os.path.join(DOC_DIR, \"*.pdf\")))\n",
        "\n",
        "if not doc_files:\n",
        "    raise RuntimeError(\"No .pdf documents found in ./data/docs. Ensure the ZIP was extracted.\")\n",
        "\n",
        "documents = []\n",
        "skipped = []\n",
        "\n",
        "for p in doc_files:\n",
        "    try:\n",
        "        reader = PdfReader(p)\n",
        "\n",
        "        # If encrypted, try empty password\n",
        "        if getattr(reader, \"is_encrypted\", False):\n",
        "            try:\n",
        "                reader.decrypt(\"\")  # common case: encrypted but no password\n",
        "            except Exception as e:\n",
        "                skipped.append((os.path.basename(p), f\"Encrypted (decrypt failed): {e}\"))\n",
        "                continue\n",
        "\n",
        "        text_parts = []\n",
        "        for page in reader.pages:\n",
        "            extracted = page.extract_text()\n",
        "            if extracted:\n",
        "                text_parts.append(extracted)\n",
        "\n",
        "        txt = \"\\n\".join(text_parts).strip()\n",
        "        if not txt:\n",
        "            skipped.append((os.path.basename(p), \"No extractable text\"))\n",
        "            continue\n",
        "\n",
        "        documents.append({\"doc_id\": os.path.basename(p), \"source\": p, \"text\": txt})\n",
        "\n",
        "    except Exception as e:\n",
        "        skipped.append((os.path.basename(p), str(e)))\n",
        "\n",
        "print(\"‚úÖ Loaded documents:\", len(documents))\n",
        "print(\"‚ö†Ô∏è Skipped:\", len(skipped))\n",
        "for name, reason in skipped[:10]:\n",
        "    print(\" -\", name, \"->\", reason)\n",
        "\n",
        "if documents:\n",
        "    print(\"\\nExample doc_id:\", documents[0][\"doc_id\"])\n",
        "    print(documents[0][\"text\"][:300])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, site\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Executable:\", sys.executable)\n",
        "print(\"Site-packages:\", site.getsitepackages())\n",
        "\n",
        "try:\n",
        "    import Crypto\n",
        "    from Crypto.Cipher import AES\n",
        "    print(\"‚úÖ pycryptodome works. Crypto version:\", Crypto.__version__)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå AES crypto NOT available:\", repr(e))\n"
      ],
      "metadata": {
        "id": "906uWTbnLMZB"
      },
      "id": "906uWTbnLMZB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install -U pip\n",
        "!python -m pip install -U pycryptodome\n"
      ],
      "metadata": {
        "id": "ADZS5ETJLQmU"
      },
      "id": "ADZS5ETJLQmU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install -q pikepdf\n",
        "\n",
        "import os\n",
        "import pikepdf\n",
        "\n",
        "src = \"./data/docs/doc5.pdf\"\n",
        "dst = \"./data/docs/doc5_decrypted.pdf\"\n",
        "\n",
        "# Try opening with empty password (common case)\n",
        "with pikepdf.open(src, password=\"\") as pdf:\n",
        "    pdf.save(dst)\n",
        "\n",
        "print(\"‚úÖ Decrypted copy saved to:\", dst)\n",
        "print(\"Now you should load doc5_decrypted.pdf instead of doc5.pdf\")\n"
      ],
      "metadata": {
        "id": "BYdHf8l1LXw7"
      },
      "id": "BYdHf8l1LXw7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "DOC_DIR = \"./data/docs\"\n",
        "doc_files = sorted(glob.glob(os.path.join(DOC_DIR, \"*.pdf\")))\n",
        "\n",
        "# If decrypted version exists, prefer it over the original\n",
        "if os.path.exists(os.path.join(DOC_DIR, \"doc5_decrypted.pdf\")):\n",
        "    doc_files = [p for p in doc_files if not p.endswith(\"doc5.pdf\")]\n",
        "\n",
        "documents = []\n",
        "skipped = []\n",
        "\n",
        "for p in doc_files:\n",
        "    try:\n",
        "        reader = PdfReader(p)\n",
        "        text_parts = []\n",
        "        for page in reader.pages:\n",
        "            t = page.extract_text()\n",
        "            if t:\n",
        "                text_parts.append(t)\n",
        "        txt = \"\\n\".join(text_parts).strip()\n",
        "        if not txt:\n",
        "            skipped.append((os.path.basename(p), \"No extractable text\"))\n",
        "            continue\n",
        "        documents.append({\"doc_id\": os.path.basename(p), \"source\": p, \"text\": txt})\n",
        "    except Exception as e:\n",
        "        skipped.append((os.path.basename(p), str(e)))\n",
        "\n",
        "print(\"‚úÖ Loaded documents:\", len(documents))\n",
        "print(\"‚ö†Ô∏è Skipped:\", skipped)\n"
      ],
      "metadata": {
        "id": "TY487-ZcLcai"
      },
      "id": "TY487-ZcLcai",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vLkoKvfRbK41",
      "metadata": {
        "id": "vLkoKvfRbK41"
      },
      "outputs": [],
      "source": [
        "# Load demo images and create lightweight text surrogates (captions) for multimodal retrieval\n",
        "import glob, os\n",
        "\n",
        "IMG_DIR = './data/images'\n",
        "img_files = sorted(glob.glob(os.path.join(IMG_DIR, '*.*')))\n",
        "img_files = [p for p in img_files if p.lower().endswith(('.png','.jpg','.jpeg','.webp'))]\n",
        "\n",
        "# Minimal captions so images participate in retrieval without requiring a vision encoder\n",
        "IMAGE_CAPTIONS = {\n",
        "    'rag_pipeline.png': 'RAG pipeline diagram: ingest, chunk, index, retrieve top-k evidence, build context, generate grounded answer, log metrics for monitoring.',\n",
        "    'retrieval_modes.png': 'Retrieval modes diagram: BM25 keyword, vector semantic, hybrid fusion, multi-hop hop-1 to hop-2 refinement.',\n",
        "}\n",
        "\n",
        "images = []\n",
        "for p in img_files:\n",
        "    fid = os.path.basename(p)\n",
        "    cap = IMAGE_CAPTIONS.get(fid, fid.replace('_',' ').replace('.png','').replace('.jpg',''))\n",
        "    images.append({'img_id': fid, 'source': p, 'text': cap})\n",
        "\n",
        "print('‚úÖ Loaded images:', len(images))\n",
        "if images:\n",
        "    print('Example image:', images[0]['img_id'])\n",
        "    print('Caption:', images[0]['text'])\n",
        "\n",
        "# Unified evidence store used by retrieval (text + images)\n",
        "items = []\n",
        "for d in documents:\n",
        "    items.append({\n",
        "        'evidence_id': d.get('doc_id') or os.path.basename(d.get('source','')),\n",
        "        'modality': 'text',\n",
        "        'source': d.get('source'),\n",
        "        'text': d.get('text','')\n",
        "    })\n",
        "for im in images:\n",
        "    items.append({\n",
        "        'evidence_id': f\"img::{im['img_id']}\",\n",
        "        'modality': 'image',\n",
        "        'source': im.get('source'),\n",
        "        'text': im.get('text','')\n",
        "    })\n",
        "\n",
        "assert len(items) > 0, 'Evidence store is empty.'\n",
        "print('‚úÖ Unified evidence items:', len(items), '(text:', len(documents), ', images:', len(images), ')')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47e549d1",
      "metadata": {
        "id": "47e549d1"
      },
      "source": [
        "# 1) Setup & environment checks\n",
        "\n",
        "This notebook includes **safe defaults** and **lightweight code examples**.  \n",
        "Replace the placeholder pieces with your Lab-3 implementation (PDF parsing, OCR, multimodal evidence, hybrid retrieval, reranking).\n",
        "\n",
        "### Install dependencies (edit as needed)\n",
        "- Core: `streamlit`, `pandas`, `numpy`, `requests`\n",
        "- Optional: `fastapi`, `uvicorn` (if you do the FastAPI extension)\n",
        "- Retrieval examples: `scikit-learn` (TF-IDF baseline), optionally `sentence-transformers` (dense embeddings)\n",
        "\n",
        "> In your team repo, always keep a clean `requirements.txt` for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "425991a8",
      "metadata": {
        "id": "425991a8"
      },
      "outputs": [],
      "source": [
        "# If running in Colab or fresh environment, uncomment installs:\n",
        "# !pip -q install streamlit pandas numpy requests scikit-learn\n",
        "# # Optional (FastAPI extension):\n",
        "# !pip -q install fastapi uvicorn pydantic\n",
        "# # Optional (dense retrieval):\n",
        "# !pip -q install sentence-transformers\n",
        "\n",
        "import os, json, time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Python OK. Working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c805bf7",
      "metadata": {
        "id": "5c805bf7"
      },
      "source": [
        "# 2) Project paths + configuration\n",
        "\n",
        "Set your project data paths and key parameters here.\n",
        "\n",
        "- Do **not** hardcode secrets (API keys) in notebooks or repos.\n",
        "- If you use a hosted LLM, read from environment variables locally.\n",
        "\n",
        "**Tip:** Keep these settings mirrored in `rag/config.py` so your Streamlit app uses the same config.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d483405",
      "metadata": {
        "id": "1d483405"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Lab4Config:\n",
        "    project_name: str = \"YOUR_PROJECT_NAME\"\n",
        "    data_dir: str = \"./data\"        # where your PDFs/images live locally\n",
        "    logs_dir: str = \"./logs\"\n",
        "    log_file: str = \"./logs/query_metrics.csv\"\n",
        "    top_k_default: int = 10\n",
        "    eval_p_at: int = 5\n",
        "    eval_r_at: int = 10\n",
        "\n",
        "cfg = Lab4Config()\n",
        "Path(cfg.logs_dir).mkdir(parents=True, exist_ok=True)\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c5030e",
      "metadata": {
        "id": "d5c5030e"
      },
      "source": [
        "# 3) Dataset wiring (project-aligned)\n",
        "\n",
        "For Lab 4, your **data, application UI, and models** must be aligned to your team project.\n",
        "\n",
        "## Required (project-aligned)\n",
        "- 2‚Äì6 PDFs\n",
        "- 5‚Äì15 images/figures/tables (if your project is multimodal)\n",
        "\n",
        "## In Lab 3 you likely had:\n",
        "- PDF text extraction (PyMuPDF)\n",
        "- OCR / captions for figures or scanned pages\n",
        "- Chunking + indexing (dense/sparse/hybrid)\n",
        "- Reranking (optional)\n",
        "- Grounded answer generation with citations\n",
        "\n",
        "### What to do here\n",
        "1. Point this notebook to your dataset folder.\n",
        "2. Load *already-prepared* chunks/evidence from Lab 3 (recommended), OR\n",
        "3. Call your Lab-3 ingestion function to rebuild the index.\n",
        "\n",
        "Below is a **minimal example** that loads plain text files as ‚Äúdocuments‚Äù so the notebook is runnable even without PDFs.\n",
        "Replace it with your Lab-3 ingestion code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f0f725",
      "metadata": {
        "id": "20f0f725"
      },
      "outputs": [],
      "source": [
        "# Minimal runnable loader for YOUR dataset (PDFs in ./data/docs)\n",
        "from pathlib import Path\n",
        "import sys, subprocess\n",
        "\n",
        "docs_dir = Path(\"data\") / \"docs\"\n",
        "docs_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- install deps if missing ---\n",
        "def pip_install(pkg: str):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "try:\n",
        "    from PyPDF2 import PdfReader\n",
        "except ModuleNotFoundError:\n",
        "    pip_install(\"PyPDF2\")\n",
        "    from PyPDF2 import PdfReader\n",
        "\n",
        "# Optional: try to decrypt AES PDFs more reliably\n",
        "# (If it still doesn't work, we'll skip encrypted PDFs)\n",
        "try:\n",
        "    import Crypto  # noqa: F401\n",
        "except ModuleNotFoundError:\n",
        "    try:\n",
        "        pip_install(\"pycryptodome\")\n",
        "    except Exception:\n",
        "        pass  # okay if install fails; we'll still load non-encrypted PDFs\n",
        "\n",
        "def load_pdf_docs(docs_path: Path):\n",
        "    items = []\n",
        "    skipped = []\n",
        "\n",
        "    for p in sorted(docs_path.glob(\"*.pdf\")):\n",
        "        try:\n",
        "            reader = PdfReader(str(p))\n",
        "\n",
        "            # Try empty password if encrypted (common case)\n",
        "            if getattr(reader, \"is_encrypted\", False):\n",
        "                try:\n",
        "                    reader.decrypt(\"\")\n",
        "                except Exception as e:\n",
        "                    skipped.append((p.name, f\"Encrypted (decrypt failed): {e}\"))\n",
        "                    continue\n",
        "\n",
        "            text_parts = []\n",
        "            for page in reader.pages:\n",
        "                t = page.extract_text()\n",
        "                if t:\n",
        "                    text_parts.append(t)\n",
        "\n",
        "            text = \"\\n\".join(text_parts).strip()\n",
        "            if not text:\n",
        "                skipped.append((p.name, \"No extractable text\"))\n",
        "                continue\n",
        "\n",
        "            items.append({\n",
        "                \"doc_id\": p.stem,         # doc1 (no .pdf)\n",
        "                \"source\": str(p),\n",
        "                \"text\": text\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            skipped.append((p.name, str(e)))\n",
        "\n",
        "    return items, skipped\n",
        "\n",
        "documents, skipped = load_pdf_docs(docs_dir)\n",
        "\n",
        "print(\"‚úÖ Loaded docs:\", len(documents))\n",
        "print(\"‚ö†Ô∏è Skipped:\", len(skipped))\n",
        "for name, reason in skipped[:10]:\n",
        "    print(\" -\", name, \"->\", reason)\n",
        "\n",
        "# show keys + one sample like the template does\n",
        "if documents:\n",
        "    print(\"\\nKeys:\", documents[0].keys())\n",
        "    print(\"Example doc_id:\", documents[0][\"doc_id\"])\n",
        "    print(documents[0][\"text\"][:300])\n",
        "else:\n",
        "    raise RuntimeError(\"No PDFs were successfully loaded from ./data/docs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip -q install pikepdf\n",
        "import pikepdf\n",
        "from pathlib import Path\n",
        "\n",
        "src = Path(\"data/docs/doc5.pdf\")\n",
        "dst = Path(\"data/docs/doc5_decrypted.pdf\")\n",
        "\n",
        "with pikepdf.open(src, password=\"\") as pdf:\n",
        "    pdf.save(dst)\n",
        "\n",
        "print(\"‚úÖ Saved:\", dst)\n"
      ],
      "metadata": {
        "id": "r6QhX0uCMNou"
      },
      "id": "r6QhX0uCMNou",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA INGESTION & CHUNKING**"
      ],
      "metadata": {
        "id": "YPlStASgMdGj"
      },
      "id": "YPlStASgMdGj"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Imports\n",
        "import os, re, glob, json, math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "!pip install PyMuPDF\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize"
      ],
      "metadata": {
        "id": "X3MGWz9mMU6i"
      },
      "id": "X3MGWz9mMU6i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"data\"\n",
        "DOC_DIR = os.path.join(DATA_DIR, \"docs\")\n",
        "FIG_DIR = os.path.join(DATA_DIR, \"images\")\n",
        "os.makedirs(DOC_DIR, exist_ok=True)\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "pdfs = sorted(glob.glob(os.path.join(DOC_DIR, \"*.pdf\")))\n",
        "imgs = sorted(glob.glob(os.path.join(FIG_DIR, \"*.*\")))\n",
        "\n",
        "print(\"PDFs:\", len(pdfs), pdfs)\n",
        "print(\"Images:\", len(imgs), imgs)\n",
        ""
      ],
      "metadata": {
        "id": "euDrmpblMoGY"
      },
      "id": "euDrmpblMoGY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y tesseract-ocr\n",
        "!pip install -q pytesseract"
      ],
      "metadata": {
        "id": "mJEXer8VM79U"
      },
      "id": "mJEXer8VM79U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Track B (Recommended)\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "@dataclass\n",
        "class TextChunk:\n",
        "    chunk_id: str\n",
        "    doc_id: str\n",
        "    page_num: int\n",
        "    text: str\n",
        "\n",
        "@dataclass\n",
        "class ImageItem:\n",
        "    item_id: str\n",
        "    path: str\n",
        "    caption: str  # simple text to make image retrieval runnable\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = s or \"\"\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def extract_pdf_pages(pdf_path: str) -> List[TextChunk]:\n",
        "    doc_id = os.path.basename(pdf_path)\n",
        "    doc = fitz.open(pdf_path)\n",
        "    out: List[TextChunk] = []\n",
        "    for i in range(len(doc)):\n",
        "        page = doc.load_page(i)\n",
        "        text = clean_text(page.get_text(\"text\"))\n",
        "        if text:\n",
        "            out.append(TextChunk(\n",
        "                chunk_id=f\"{doc_id}::p{i+1}\",\n",
        "                doc_id=doc_id,\n",
        "                page_num=i+1,\n",
        "                text=text\n",
        "            ))\n",
        "    return out\n",
        "\n",
        "def load_images_track_b(fig_dir: str) -> List[ImageItem]:\n",
        "    items: List[ImageItem] = []\n",
        "    print(f\"Scanning images in {fig_dir} with OCR...\")\n",
        "\n",
        "    for p in sorted(glob.glob(os.path.join(fig_dir, \"*.*\"))):\n",
        "        base = os.path.basename(p)\n",
        "\n",
        "        # 1. Generate Caption (Filename based)\n",
        "        simple_caption = os.path.splitext(base)[0].replace(\"_\", \" \")\n",
        "\n",
        "        # 2. Run OCR (Tesseract) to get text inside the image\n",
        "        try:\n",
        "            image = Image.open(p)\n",
        "            ocr_text = pytesseract.image_to_string(image).strip()\n",
        "            # Clean up OCR noise (optional)\n",
        "            ocr_text = re.sub(r\"\\s+\", \" \", ocr_text)\n",
        "        except Exception as e:\n",
        "            print(f\"OCR Failed for {base}: {e}\")\n",
        "            ocr_text = \"\"\n",
        "\n",
        "        # 3. Combine for Evidence (Track B Requirement)\n",
        "        # evidence_text = Caption + OCR\n",
        "        final_text = f\"Caption: {simple_caption}. Content: {ocr_text}\"\n",
        "\n",
        "        items.append(ImageItem(item_id=base, path=p, caption=final_text))\n",
        "\n",
        "    return items\n",
        "\n",
        "# Run ingestion\n",
        "page_chunks: List[TextChunk] = []\n",
        "for p in pdfs:\n",
        "    page_chunks.extend(extract_pdf_pages(p))\n",
        "\n",
        "image_items = load_images_track_b(FIG_DIR)\n",
        "\n",
        "print(\"Total text chunks:\", len(page_chunks))\n",
        "print(\"Total images:\", len(image_items))\n",
        "print(\"Sample text chunk:\", page_chunks[0].chunk_id, page_chunks[0].text[:180])\n",
        "print(\"Sample image item:\", image_items[0])\n",
        "\n",
        "# --- Deliverable Output ---\n",
        "\n",
        "print(\"\\n=== Deliverable: Extracted PDF Chunk ===\")\n",
        "if page_chunks:\n",
        "    chunk = page_chunks[0]\n",
        "    print(f\"Chunk ID:   {chunk.chunk_id}\")\n",
        "    print(f\"Source Doc: {chunk.doc_id}\")\n",
        "    print(f\"Page Num:   {chunk.page_num}\")\n",
        "    print(f\"Text Content (First 300 chars):\\n{chunk.text[:300]}...\")\n",
        "else:\n",
        "    print(\"‚ùå No PDF chunks found.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(\"\\n=== Deliverable: Extracted Image Evidence ===\")\n",
        "if image_items:\n",
        "    item = image_items[0]\n",
        "    print(f\"Image ID: {item.item_id}\")\n",
        "    print(f\"Path:     {item.path}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Full Evidence Text (Caption + OCR):\\n{item.caption}\")\n",
        "    # Note: item.caption now holds \"Caption: [filename]. Content: [OCR Text]\"\n",
        "else:\n",
        "    print(\"‚ùå No images found.\")\n"
      ],
      "metadata": {
        "id": "V9o0qHJvNCZK"
      },
      "id": "V9o0qHJvNCZK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d9b108aa",
      "metadata": {
        "id": "d9b108aa"
      },
      "source": [
        "# 4) Mini Gold Set (Q1‚ÄìQ5) ‚Äî Required\n",
        "\n",
        "Create **5 project-relevant queries** and define a simple evidence rubric.\n",
        "\n",
        "- **Q1‚ÄìQ3:** typical project queries (answerable using evidence)\n",
        "- **Q4:** multimodal evidence query (table/figure heavy, OCR/captions should help)\n",
        "- **Q5:** missing-evidence or ambiguous query (must trigger safe behavior)\n",
        "\n",
        "For each query, define:\n",
        "- `gold_evidence_ids`: list of evidence identifiers that are relevant (doc_id/page/fig id)\n",
        "- `answer_criteria`: 1‚Äì2 bullets\n",
        "- `citation_format`: how you will cite (e.g., `[Doc1 p3]`, `[fig2]`)\n",
        "\n",
        "This enables **consistent evaluation** and makes logging meaningful.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "print(\"PDF docs:\", [p.name for p in Path(\"data/docs\").glob(\"*.pdf\")])\n",
        "print(\"Images:\", [p.name for p in Path(\"data/images\").glob(\"*.png\")])\n"
      ],
      "metadata": {
        "id": "blggXngxOAlv"
      },
      "id": "blggXngxOAlv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c131bc70",
      "metadata": {
        "id": "c131bc70"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "MISSING_EVIDENCE_PHRASE = \"Not enough evidence in the retrieved context.\"\n",
        "\n",
        "mini_gold = [\n",
        "    {\n",
        "        \"query_id\": \"Q1\",\n",
        "        \"question\": \"According to doc1.pdf, what are State Data Breach Notification Laws and what compliance-based exception(s) are mentioned?\",\n",
        "        \"gold_evidence_ids\": [\"doc1.pdf\"],\n",
        "        \"answer_criteria\": [\n",
        "            \"Defines/describes state data breach notification laws\",\n",
        "            \"Mentions an exception based on compliance with other laws (e.g., HIPAA/GLBA) if stated\",\n",
        "            \"Includes a citation\"\n",
        "        ],\n",
        "        \"citation_format\": \"[doc_id]\",\n",
        "        \"query_type\": \"answerable\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q2\",\n",
        "        \"question\": \"From the cyber_kill_chain.png diagram, list the main stages shown in the Cyber Kill Chain in order.\",\n",
        "        \"gold_evidence_ids\": [\"cyber_kill_chain.png\"],\n",
        "        \"answer_criteria\": [\n",
        "            \"Lists the stages shown in the diagram\",\n",
        "            \"Keeps the correct order\",\n",
        "            \"Includes a citation\"\n",
        "        ],\n",
        "        \"citation_format\": \"[evidence_id]\",\n",
        "        \"query_type\": \"answerable\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q3\",\n",
        "        \"question\": \"Based on soc2_requirements.png, what are the key SOC 2 Trust Service Criteria categories shown?\",\n",
        "        \"gold_evidence_ids\": [\"soc2_requirements.png\"],\n",
        "        \"answer_criteria\": [\n",
        "            \"Names the categories visible in the image\",\n",
        "            \"Does not invent categories not shown\",\n",
        "            \"Includes a citation\"\n",
        "        ],\n",
        "        \"citation_format\": \"[evidence_id]\",\n",
        "        \"query_type\": \"answerable\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q4\",\n",
        "        \"question\": \"Using impact_likelihood_matrix.png, which area represents the highest risk and why?\",\n",
        "        \"gold_evidence_ids\": [\"impact_likelihood_matrix.png\"],\n",
        "        \"answer_criteria\": [\n",
        "            \"Identifies highest risk as the region where both impact and likelihood are highest (or equivalent label)\",\n",
        "            \"Explains using the matrix meaning (risk increases with likelihood and impact)\",\n",
        "            \"Includes a citation\"\n",
        "        ],\n",
        "        \"citation_format\": \"[evidence_id]\",\n",
        "        \"query_type\": \"multimodal\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q5\",\n",
        "        \"question\": \"Who won the FIFA World Cup in 2050?\",\n",
        "        \"gold_evidence_ids\": [],  # ‚úÖ keep as list (type-stable)\n",
        "        \"answer_criteria\": [\n",
        "            f'Returns exactly the missing-evidence phrase: \"{MISSING_EVIDENCE_PHRASE}\"',\n",
        "            \"Does not claim a winner\",\n",
        "            \"Does not cite any evidence\"\n",
        "        ],\n",
        "        \"citation_format\": \"\",\n",
        "        \"query_type\": \"missing_evidence\",\n",
        "        \"expected_safe_answer\": MISSING_EVIDENCE_PHRASE\n",
        "    },\n",
        "]\n",
        "\n",
        "pd.DataFrame(mini_gold)[[\"query_id\", \"question\", \"gold_evidence_ids\", \"query_type\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rc7GAupjbK43",
      "metadata": {
        "id": "Rc7GAupjbK43"
      },
      "outputs": [],
      "source": [
        "# Task: Mini gold set (evidence IDs) for evaluation\n",
        "# Evidence IDs refer to files under ./data/docs or ./data/images\n",
        "# Image evidence uses prefix img::\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "mini_gold = [\n",
        "    {\n",
        "        'query_id': 'Q1',\n",
        "        'question': 'According to doc1.pdf, what are State Data Breach Notification Laws and what compliance exception is mentioned?',\n",
        "        'gold_evidence_ids': ['doc1.pdf']\n",
        "    },\n",
        "    {\n",
        "        'query_id': 'Q2',\n",
        "        'question': 'What are the main stages shown in the Cyber Kill Chain diagram?',\n",
        "        'gold_evidence_ids': ['img::cyber_kill_chain.png']\n",
        "    },\n",
        "    {\n",
        "        'query_id': 'Q3',\n",
        "        'question': 'What are the SOC 2 Trust Service Criteria categories shown in the SOC 2 diagram?',\n",
        "        'gold_evidence_ids': ['img::soc2_requirements.png']\n",
        "    },\n",
        "    {\n",
        "        'query_id': 'Q4',\n",
        "        'question': 'From the impact-likelihood risk matrix, which area represents the highest risk?',\n",
        "        'gold_evidence_ids': ['img::impact_likelihood_matrix.png']\n",
        "    },\n",
        "    {\n",
        "        'query_id': 'Q5',\n",
        "        'question': 'Who won the FIFA World Cup in 2050?',\n",
        "        'gold_evidence_ids': []   # Missing-evidence case (must trigger safe behavior)\n",
        "    },\n",
        "    {\n",
        "        'query_id': 'Q6',\n",
        "        'question': 'What are the five core functions shown in the NIST Cybersecurity Framework diagram?',\n",
        "        'gold_evidence_ids': ['img::nist_framework.png']\n",
        "    },\n",
        "]\n",
        "\n",
        "pd.DataFrame(mini_gold)[['query_id','question','gold_evidence_ids']]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6da3e87e",
      "metadata": {
        "id": "6da3e87e"
      },
      "source": [
        "# 5) Retrieval + Answer Function (Reuse Lab 3)\n",
        "\n",
        "Below is a **baseline TF‚ÄëIDF retriever** so this notebook is runnable.\n",
        "Replace with your Lab-3 retrieval stack:\n",
        "- dense (SentenceTransformers + FAISS/Chroma)\n",
        "- sparse (BM25)\n",
        "- hybrid fusion\n",
        "- optional reranking\n",
        "\n",
        "### Required output contract (recommended)\n",
        "Your retrieval function should return a list of evidence items:\n",
        "- `chunk_id` or `doc_id`\n",
        "- `source`\n",
        "- `score`\n",
        "- `citation_tag` (e.g., `[Doc1 p3]`, `[fig2]`)\n",
        "- `text` (the evidence text shown to users)\n",
        "\n",
        "Your answer function must enforce:\n",
        "- **Citations for claims**\n",
        "- If missing evidence: **return exactly**  \n",
        "  `Not enough evidence in the retrieved context.`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f60c59",
      "metadata": {
        "id": "e1f60c59"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Build a simple TF-IDF index over documents (demo baseline)\n",
        "corpus = [d[\"text\"] for d in documents]\n",
        "doc_ids = [d[\"doc_id\"] for d in documents]\n",
        "sources = [d[\"source\"] for d in documents]\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "def retrieve_tfidf(question: str, top_k: int = 5):\n",
        "    q = vectorizer.transform([question])\n",
        "    sims = cosine_similarity(q, X).ravel()\n",
        "    idxs = np.argsort(-sims)[:top_k]\n",
        "    evidence = []\n",
        "    for rank, i in enumerate(idxs):\n",
        "        evidence.append({\n",
        "            \"chunk_id\": doc_ids[i],\n",
        "            \"source\": sources[i],\n",
        "            \"score\": float(sims[i]),\n",
        "            \"citation_tag\": f\"[{doc_ids[i]}]\",\n",
        "            \"text\": corpus[i][:800]  # truncate for UI\n",
        "        })\n",
        "    return evidence\n",
        "\n",
        "MISSING_EVIDENCE_MSG = \"Not enough evidence in the retrieved context.\"\n",
        "\n",
        "def generate_answer_stub(question: str, evidence: list):\n",
        "    \"\"\"Replace with your LLM/VLM generation.\n",
        "    For this template we produce a simple grounded response.\n",
        "    \"\"\"\n",
        "    if not evidence or max(e.get(\"score\", 0.0) for e in evidence) < 0.05:\n",
        "        return MISSING_EVIDENCE_MSG\n",
        "\n",
        "    # Minimal grounded \"answer\" example: summarize top evidence\n",
        "    top = evidence[0]\n",
        "    answer = (\n",
        "        f\"Based on the retrieved evidence {top['citation_tag']}, \"\n",
        "        f\"the system should ground its response in retrieved context and cite sources. \"\n",
        "        f\"If evidence is missing, it must respond with: '{MISSING_EVIDENCE_MSG}'. \"\n",
        "        f\"{top['citation_tag']}\"\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "# Quick test\n",
        "test_q = mini_gold[0][\"question\"]\n",
        "ev = retrieve_tfidf(test_q, top_k=3)\n",
        "print(\"Top evidence:\", ev[0][\"chunk_id\"], ev[0][\"score\"])\n",
        "print(\"Answer:\", generate_answer_stub(test_q, ev))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fixed-size Chunking Strategy (Sliding Window Chunk)**"
      ],
      "metadata": {
        "id": "m-UxevgRPgjm"
      },
      "id": "m-UxevgRPgjm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking knobs (for fixed-size chunking ablation)\n",
        "CHUNK_SIZE    = 900   # characters per chunk\n",
        "CHUNK_OVERLAP = 150   # overlap characters\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 0"
      ],
      "metadata": {
        "id": "XumKw417PmTr"
      },
      "id": "XumKw417PmTr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_fixed_size_chunks(pdf_path: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[TextChunk]:\n",
        "    doc_id = os.path.basename(pdf_path)\n",
        "    doc = fitz.open(pdf_path)\n",
        "    full_text = \"\"\n",
        "    for page in doc:\n",
        "        full_text += clean_text(page.get_text(\"text\")) + \" \"\n",
        "\n",
        "    # Sliding window slicing\n",
        "    chunks = []\n",
        "    for i in range(0, len(full_text), chunk_size - overlap):\n",
        "        window = full_text[i : i + chunk_size]\n",
        "        if len(window) > 50: # Filter tiny chunks\n",
        "            chunks.append(TextChunk(\n",
        "                chunk_id=f\"{doc_id}::span{i}-{i+len(window)}\",\n",
        "                doc_id=doc_id,\n",
        "                page_num=0, # Logical chunk, not page bound\n",
        "                text=window\n",
        "            ))\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "Vceu0t3GPq5w"
      },
      "id": "Vceu0t3GPq5w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Retrieval (TF‚ÄëIDF)**\n",
        "We build two TF‚ÄëIDF indexes:\n",
        "\n",
        "- One over **PDF text chunks**\n",
        "- One over **image captions**\n",
        "\n",
        "Retrieval returns the top‚Äëk results with similarity scores."
      ],
      "metadata": {
        "id": "8p3YrwfLQeEE"
      },
      "id": "8p3YrwfLQeEE"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tfidf_index_text(chunks: List[TextChunk]):\n",
        "    corpus = [c.text for c in chunks]\n",
        "    vec = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
        "    X = vec.fit_transform(corpus)\n",
        "    X = normalize(X)\n",
        "    return vec, X\n",
        "\n",
        "def build_tfidf_index_images(items: List[ImageItem]):\n",
        "    corpus = [it.caption for it in items]\n",
        "    vec = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
        "    X = vec.fit_transform(corpus)\n",
        "    X = normalize(X)\n",
        "    return vec, X\n",
        "\n",
        "text_vec, text_X = build_tfidf_index_text(page_chunks)\n",
        "img_vec, img_X = build_tfidf_index_images(image_items)\n",
        "\n",
        "def tfidf_retrieve(query: str, vec: TfidfVectorizer, X, top_k: int = 5):\n",
        "    q = vec.transform([query])\n",
        "    q = normalize(q)\n",
        "    scores = (X @ q.T).toarray().ravel()\n",
        "    idx = np.argsort(-scores)[:top_k]\n",
        "    return [(int(i), float(scores[i])) for i in idx]\n",
        "\n",
        "print(\"‚úÖ Indexes built.\")\n",
        "\n",
        "# Inspect built indexes by listing first 5 as a sample\n",
        "print(f\"--- Text Index ({len(page_chunks)} items) ---\")\n",
        "for i, chunk in enumerate(page_chunks[:5]):  # Print first 5 as a sample\n",
        "    # Assuming 'chunk' has a 'source_doc' or similar attribute, otherwise just print text\n",
        "    preview = chunk.text[:50].replace(\"\\n\", \" \") + \"...\"\n",
        "    print(f\"ID {i}: {preview}\")\n",
        "\n",
        "print(f\"\\n--- Image Index ({len(image_items)} items) ---\")\n",
        "for i, item in enumerate(image_items[:5]):\n",
        "    print(f\"ID {i}: {item.caption} (File: {item.item_id})\")"
      ],
      "metadata": {
        "id": "P0UUwRLeQtsz"
      },
      "id": "P0UUwRLeQtsz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build Dense Retrieval and Figure Index**"
      ],
      "metadata": {
        "id": "uKRZiVuvQ60C"
      },
      "id": "uKRZiVuvQ60C"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers\n",
        "!pip install -q sentence-transformers faiss-cpu"
      ],
      "metadata": {
        "id": "iRZ2DbU6Q9FK"
      },
      "id": "iRZ2DbU6Q9FK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Retrieval knobs\n",
        "TOP_K_TEXT     = 5    # candidate text chunks\n",
        "TOP_K_IMAGES   = 3    # candidate images (based on captions/filenames)\n",
        "TOP_K_EVIDENCE = 8    # final evidence items used in the context"
      ],
      "metadata": {
        "id": "jbbrEDvfRG6e"
      },
      "id": "jbbrEDvfRG6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Create embeddings\n",
        "corpus_text = [c.text for c in page_chunks]\n",
        "# Remove convert_to_tensor=True so we get a NumPy array for FAISS\n",
        "corpus_embeddings = model.encode(corpus_text)\n",
        "\n",
        "# Build FAISS Index\n",
        "d = corpus_embeddings.shape[1]  # Dimension of embeddings (e.g., 384)\n",
        "index_dense = faiss.IndexFlatL2(d) # L2 distance (Euclidean)\n",
        "index_dense.add(corpus_embeddings)\n",
        "\n",
        "print(f\"‚úÖ Dense Index built with {index_dense.ntotal} vectors.\")\n",
        "\n",
        "# Embed the captions from your image_items list\n",
        "corpus_caption = [item.caption for item in image_items]\n",
        "caption_embeddings = model.encode(corpus_caption, convert_to_tensor=False)\n",
        "\n",
        "# Build FAISS Index for Image Captions\n",
        "d_cap = caption_embeddings.shape[1] # Dimension = 384\n",
        "index_captions = faiss.IndexFlatL2(d_cap)\n",
        "index_captions.add(caption_embeddings)\n",
        "\n",
        "print(f\"‚úÖ Approach 1 (Captions): Indexed {index_captions.ntotal} images via text.\")\n",
        "\n",
        "def dense_retrieve(query, top_k=TOP_K_TEXT):\n",
        "    # Encode query to numpy. Wrap in list [query] to ensure (1, d) shape.\n",
        "    query_emb = model.encode([query])\n",
        "\n",
        "    # Search FAISS\n",
        "    distances, indices = index_dense.search(query_emb, top_k)\n",
        "\n",
        "    # Return indices\n",
        "    return [(int(idx), float(dist)) for idx, dist in zip(indices[0], distances[0])]\n",
        "\n",
        "def retrieve_images_by_caption(query: str, top_k=TOP_K_IMAGES):\n",
        "    # Embed query using the SAME text model\n",
        "    q_emb = model.encode([query])\n",
        "    distances, indices = index_captions.search(q_emb, top_k)\n",
        "\n",
        "    # Return matched ImageItems\n",
        "    results = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        if idx < 0: continue # FAISS returns -1 if not found\n",
        "        results.append((image_items[idx], float(dist)))\n",
        "    return results\n",
        "\n",
        "# Validation by checking vocabulary size\n",
        "print(f\"Text Dictionary Size: {len(text_vec.vocabulary_)}\")\n",
        "print(f\"Image Dictionary Size: {len(img_vec.vocabulary_)}\")"
      ],
      "metadata": {
        "id": "VUGC-uMIRI0e"
      },
      "id": "VUGC-uMIRI0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build evidence context**\n",
        "We assemble a compact context string + list of image paths.\n",
        "\n",
        "**Guidelines for good context:**\n",
        "\n",
        "- Keep snippets short (100‚Äì300 chars)\n",
        "- Always include chunk IDs so you can cite evidence\n",
        "- Attach images that are likely relevant"
      ],
      "metadata": {
        "id": "5V-cAFFXRkCU"
      },
      "id": "5V-cAFFXRkCU"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Fusion / retrieval hyperparameters ---\n",
        "TOP_K_TEXT = 5\n",
        "TOP_K_IMAGES = 3\n",
        "TOP_K_EVIDENCE = 5\n",
        "ALPHA = 0.6   # 0.5 = balanced, >0.5 favors text, <0.5 favors images\n"
      ],
      "metadata": {
        "id": "0OxS7sbjSEEH"
      },
      "id": "0OxS7sbjSEEH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _normalize_scores(pairs):\n",
        "    \"\"\"Min-max normalize a list of (idx, score) to [0,1].\n",
        "    If all scores equal, returns 1.0 for each item (so ordering stays stable).\n",
        "    \"\"\"\n",
        "    if not pairs:\n",
        "        return []\n",
        "    scores = [s for _, s in pairs]\n",
        "    lo, hi = min(scores), max(scores)\n",
        "    if abs(hi - lo) < 1e-12:\n",
        "        return [(i, 1.0) for i, _ in pairs]\n",
        "    return [(i, (s - lo) / (hi - lo)) for i, s in pairs]\n",
        "\n",
        "\n",
        "def build_context(\n",
        "    question: str,\n",
        "    top_k_text: int = TOP_K_TEXT,\n",
        "    top_k_images: int = TOP_K_IMAGES,\n",
        "    top_k_evidence: int = TOP_K_EVIDENCE,\n",
        "    alpha: float = ALPHA,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Build a multimodal context block for the question.\n",
        "\n",
        "    Students:\n",
        "    - `top_k_text` / `top_k_images` control *candidate retrieval* per modality.\n",
        "    - `top_k_evidence` controls the *final context size*.\n",
        "    - `alpha` controls fusion: higher = prefer text evidence, lower = prefer images.\n",
        "\n",
        "    This function returns:\n",
        "    - `context`: a text block with the selected evidence (what you pass to an LLM)\n",
        "    - `image_paths`: paths of images selected as evidence\n",
        "    - `evidence`: structured evidence list (recommended for your report)\n",
        "    \"\"\"\n",
        "    # 1) Retrieve candidates from each modality\n",
        "    text_hits = tfidf_retrieve(question, text_vec, text_X, top_k=top_k_text)   # [(idx, score), ...]\n",
        "    img_hits  = tfidf_retrieve(question, img_vec,  img_X,  top_k=top_k_images)\n",
        "\n",
        "    # 2) Normalize scores per modality and fuse with ALPHA\n",
        "    text_norm = _normalize_scores(text_hits)\n",
        "    img_norm  = _normalize_scores(img_hits)\n",
        "\n",
        "    fused = []\n",
        "    for idx, s in text_norm:\n",
        "        ch = page_chunks[idx]\n",
        "        fused.append({\n",
        "            \"modality\": \"text\",\n",
        "            \"id\": ch.chunk_id,\n",
        "            \"raw_score\": float(dict(text_hits).get(idx, 0.0)),\n",
        "            \"fused_score\": float(alpha * s),\n",
        "            \"text\": ch.text,\n",
        "            \"path\": None,\n",
        "        })\n",
        "\n",
        "    for idx, s in img_norm:\n",
        "        it = image_items[idx]\n",
        "        fused.append({\n",
        "            \"modality\": \"image\",\n",
        "            \"id\": it.item_id,\n",
        "            \"raw_score\": float(dict(img_hits).get(idx, 0.0)),\n",
        "            \"fused_score\": float((1.0 - alpha) * s),\n",
        "            \"text\": it.caption,     # we retrieve on caption/filename text\n",
        "            \"path\": it.path,\n",
        "        })\n",
        "\n",
        "    # 3) Pick top fused evidence\n",
        "    fused = sorted(fused, key=lambda d: d[\"fused_score\"], reverse=True)[:top_k_evidence]\n",
        "\n",
        "    # 4) Build the context string (what you feed into a generator/LLM)\n",
        "    ctx_lines = []\n",
        "    image_paths = []\n",
        "    for ev in fused:\n",
        "        if ev[\"modality\"] == \"text\":\n",
        "            snippet = (ev[\"text\"] or \"\")[:260].replace(\"\\n\", \" \")\n",
        "            ctx_lines.append(f\"[TEXT | {ev['id']} | fused={ev['fused_score']:.3f}] {snippet}\")\n",
        "        else:\n",
        "            ctx_lines.append(f\"[IMAGE | {ev['id']} | fused={ev['fused_score']:.3f}] caption={ev['text']}\")\n",
        "            image_paths.append(ev[\"path\"])\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"context\": \"\\n\".join(ctx_lines),\n",
        "        \"image_paths\": image_paths,\n",
        "        \"text_hits\": text_hits,\n",
        "        \"img_hits\": img_hits,\n",
        "        \"evidence\": fused,\n",
        "        \"alpha\": alpha,\n",
        "        \"top_k_text\": top_k_text,\n",
        "        \"top_k_images\": top_k_images,\n",
        "        \"top_k_evidence\": top_k_evidence,\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Demo: what retrieval returns for one query ---\n",
        "ctx_demo = build_context(mini_gold[0][\"question\"])\n",
        "print(ctx_demo[\"context\"])\n",
        "print(\"Images:\", ctx_demo[\"image_paths\"])\n",
        "print(\"Fusion alpha:\", ctx_demo[\"alpha\"])"
      ],
      "metadata": {
        "id": "Z-fSpR34Rx4E"
      },
      "id": "Z-fSpR34Rx4E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reranking**"
      ],
      "metadata": {
        "id": "v636mCk6SUzq"
      },
      "id": "v636mCk6SUzq"
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Load a standard reranking model (trained on MS MARCO)\n",
        "# This model outputs a score (higher is better, usually unbounded but often -10 to 10)\n",
        "print(\"Loading Reranker...\")\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "print(\"‚úÖ Reranker loaded.\")"
      ],
      "metadata": {
        "id": "RO1exXSkSWSw"
      },
      "id": "RO1exXSkSWSw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_scores(hits):\n",
        "    \"\"\"Normalizes a list of (idx, score) to 0..1 range.\"\"\"\n",
        "    if not hits: return []\n",
        "    scores = [s for _, s in hits]\n",
        "    min_s, max_s = min(scores), max(scores)\n",
        "    if max_s == min_s: return [(i, 1.0) for i, _ in hits]\n",
        "    return [(i, (s - min_s) / (max_s - min_s)) for i, s in hits]\n",
        "\n",
        "def get_retrieval_results(query: str, method: str, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Retrieves candidate chunks based on the specified method.\n",
        "    Returns a list of (chunk_index, score).\n",
        "    \"\"\"\n",
        "    # 1. SPARSE ONLY\n",
        "    if method == \"Sparse Only\":\n",
        "        return tfidf_retrieve(query, text_vec, text_X, top_k=top_k)\n",
        "\n",
        "    # 2. DENSE ONLY\n",
        "    if method == \"Dense Only\":\n",
        "        # Assumes dense_retrieve exists from previous step\n",
        "        return dense_retrieve(query, top_k=top_k)\n",
        "\n",
        "    # 3. HYBRID (Sparse + Dense)\n",
        "    if method == \"Hybrid\" or method == \"Hybrid + Rerank\" or method == \"Multimodal\":\n",
        "        # Retrieve more candidates (e.g., top_k * 2) from both to ensure overlap\n",
        "        sparse_hits = tfidf_retrieve(query, text_vec, text_X, top_k=top_k*2)\n",
        "        dense_hits = dense_retrieve(query, top_k=top_k*2)\n",
        "\n",
        "        # Create a dict to fuse scores: {idx: fused_score}\n",
        "        fusion_map = {}\n",
        "\n",
        "        # Normalize and weigh (Alpha=0.5 usually works well for Hybrid)\n",
        "        for idx, score in normalize_scores(sparse_hits):\n",
        "            fusion_map[idx] = fusion_map.get(idx, 0) + (0.5 * score)\n",
        "\n",
        "        for idx, score in normalize_scores(dense_hits):\n",
        "            fusion_map[idx] = fusion_map.get(idx, 0) + (0.5 * score)\n",
        "\n",
        "        # Sort by fused score\n",
        "        hybrid_results = sorted(fusion_map.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # If just Hybrid, return top_k\n",
        "        if method == \"Hybrid\":\n",
        "            return hybrid_results[:top_k]\n",
        "\n",
        "        # 4. RERANKING (Re-score the hybrid candidates)\n",
        "        # We take the top 20 hybrid candidates and rerank them\n",
        "        candidates = hybrid_results[:20]\n",
        "\n",
        "        # Prepare pairs for CrossEncoder: [[query, doc_text], ...]\n",
        "        pairs = []\n",
        "        for idx, _ in candidates:\n",
        "            pairs.append([query, page_chunks[idx].text])\n",
        "\n",
        "        # Predict scores\n",
        "        rerank_scores = reranker.predict(pairs)\n",
        "\n",
        "        # Attach new scores to indices\n",
        "        reranked_results = []\n",
        "        for i, (idx, _) in enumerate(candidates):\n",
        "            reranked_results.append((idx, float(rerank_scores[i])))\n",
        "\n",
        "        # Sort by new reranker score\n",
        "        final_ranked = sorted(reranked_results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return final_ranked[:top_k]\n",
        "\n",
        "    return []"
      ],
      "metadata": {
        "id": "2aeln-s1Sdld"
      },
      "id": "2aeln-s1Sdld",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **‚ÄúGenerator‚Äù (simple, offline)**\n",
        "To keep this notebook runnable anywhere, we implement **a lightweight extractive generator**:\n",
        "\n",
        "- It returns the top evidence lines\n",
        "- In addition , we implement LLM Call with HF local model\n",
        "- LLM call with API"
      ],
      "metadata": {
        "id": "HypDPKvCSje8"
      },
      "id": "HypDPKvCSje8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fusion knob (text vs images)\n",
        "ALPHA = 0.5  # 0.0 = images dominate, 1.0 = text dominates\n",
        ""
      ],
      "metadata": {
        "id": "O0zzNvYwSswe"
      },
      "id": "O0zzNvYwSswe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method 1: Lightweight extractive generator\n",
        "\n",
        "def simple_extractive_answer(question: str, context: str) -> str:\n",
        "    lines = context.splitlines()\n",
        "    if not lines:\n",
        "        return \"I don't know (no evidence retrieved).\"\n",
        "    # Return top 2 evidence lines as a \"grounded\" answer\n",
        "    return (\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        \"Grounded answer (extractive):\\n\"\n",
        "        + \"\\n\".join(lines[:2])\n",
        "    )\n",
        "\n",
        "def run_query(qobj, top_k_text=TOP_K_TEXT, top_k_images=TOP_K_IMAGES, top_k_evidence=TOP_K_EVIDENCE, alpha=ALPHA) -> Dict[str, Any]:\n",
        "    question = qobj[\"question\"]\n",
        "    ctx = build_context(question, top_k_text=top_k_text, top_k_images=top_k_images, top_k_evidence=top_k_evidence, alpha=alpha)\n",
        "    answer = simple_extractive_answer(question, ctx[\"context\"])\n",
        "    return {\n",
        "        \"id\": qobj[\"query_id\"], # Fixed: changed from \"id\" to \"query_id\"\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"context\": ctx[\"context\"],\n",
        "        \"image_paths\": ctx[\"image_paths\"],\n",
        "        \"text_hits\": ctx[\"text_hits\"],\n",
        "        \"img_hits\": ctx[\"img_hits\"],\n",
        "    }\n",
        "\n",
        "results = [run_query(q) for q in mini_gold]\n",
        "for r in results:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(r[\"id\"], r[\"question\"])\n",
        "    print(r[\"answer\"][:500])\n",
        "    print(\"Images:\", [os.path.basename(p) for p in r[\"image_paths\"]])"
      ],
      "metadata": {
        "id": "MrilEgoISwW7"
      },
      "id": "MrilEgoISwW7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generator using LLM (API Call) with model gemini-2.5-flash**"
      ],
      "metadata": {
        "id": "wd_60GT8S0nZ"
      },
      "id": "wd_60GT8S0nZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: LLM extractive generator (API Call)\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- SETUP LLM ---\n",
        "# Set up secret key on the left side bar\n",
        "try:\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "except Exception:\n",
        "    api_key = \"PASTE_YOUR_KEY_HERE\"\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = api_key\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "def generate_llm_answer(question: str, context: str) -> str:\n",
        "    \"\"\"Generates an answer using an LLM (Gemini) based on the provided context.\"\"\"\n",
        "\n",
        "    # 1. Check for empty context\n",
        "    if not context or not context.strip():\n",
        "        return \"Not enough evidence in the retrieved context.\"\n",
        "\n",
        "    # 2. Define the model\n",
        "    # Using gemini-2.5-flash as it is widely available and free-tier friendly\n",
        "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "    # 3. Construct the prompt\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful assistant for a Multimodal RAG system.\n",
        "    Use the following retrieved context (text chunks and image descriptions) to answer the user's question.\n",
        "\n",
        "    RULES:\n",
        "    1. Answer ONLY using the provided context. If the answer is not in the context, say \"Not enough evidence in the retrieved context.\"\n",
        "    2. Cite your sources! When you use information, append the source ID like [TEXT | doc1.pdf::p1] or [IMAGE | figure1.png].\n",
        "    3. Be concise and direct.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION:\n",
        "    {question}\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "\n",
        "    # 4. Call the API\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"LLM Generation Error: {str(e)} (Check your API Key)\"\n",
        "\n",
        "# --- UPDATED RUN_QUERY ---\n",
        "def run_query(qobj, top_k_text=TOP_K_TEXT, top_k_images=TOP_K_IMAGES, top_k_evidence=TOP_K_EVIDENCE, alpha=ALPHA) -> Dict[str, Any]:\n",
        "    question = qobj[\"question\"]\n",
        "\n",
        "    # 1. Retrieve and Build Context\n",
        "    ctx = build_context(question, top_k_text=top_k_text, top_k_images=top_k_images, top_k_evidence=top_k_evidence, alpha=alpha)\n",
        "\n",
        "    # 2. Generate Answer with LLM (Replaces simple_extractive_answer)\n",
        "    answer = generate_llm_answer(question, ctx[\"context\"])\n",
        "\n",
        "    return {\n",
        "        \"id\": qobj[\"query_id\"],\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"context\": ctx[\"context\"],\n",
        "        \"image_paths\": ctx[\"image_paths\"],\n",
        "        \"text_hits\": ctx[\"text_hits\"],\n",
        "        \"img_hits\": ctx[\"img_hits\"],\n",
        "    }\n",
        "\n",
        "# --- EXECUTION ---\n",
        "results = [run_query(q) for q in mini_gold]\n",
        "\n",
        "for r in results:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"[{r['id']}] Question: {r['question']}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"LLM Answer:\\n{r['answer']}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Context Images:\", [os.path.basename(p) for p in r[\"image_paths\"]])"
      ],
      "metadata": {
        "id": "as4FwvgvS7cO"
      },
      "id": "as4FwvgvS7cO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generator using HuggingFace LLM (local) with flan-t5-large**"
      ],
      "metadata": {
        "id": "Mb7fFSDnTA81"
      },
      "id": "Mb7fFSDnTA81"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "rtIaY57ZTDrZ"
      },
      "id": "rtIaY57ZTDrZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 3: HuggingFace Local\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the local model (for extractive RAG)\n",
        "print(\"Loading local model...\")\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    # model=\"google/flan-t5-large\",\n",
        "    model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "print(\"‚úÖ Model loaded.\")\n",
        ""
      ],
      "metadata": {
        "id": "ZnXJJ_IqTJBd"
      },
      "id": "ZnXJJ_IqTJBd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_extractive_answer(question: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces simple_extractive_answer with a local LLM generation.\n",
        "    \"\"\"\n",
        "    if not context or not context.strip():\n",
        "        return \"I don't know (no evidence retrieved).\"\n",
        "\n",
        "    tokenizer = llm_pipeline.tokenizer # Access tokenizer from the pipeline\n",
        "    max_context_tokens = 1500 # Safe limit (2048 total - 400 new - ~148 buffer)\n",
        "\n",
        "    # Tokenize the context\n",
        "    tokenized_context = tokenizer(context, truncation=False, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "    # If context is too long, slice it\n",
        "    if tokenized_context.shape[1] > max_context_tokens:\n",
        "        # Keep the first 1500 tokens\n",
        "        tokenized_context = tokenized_context[:, :max_context_tokens]\n",
        "        # Decode back to string\n",
        "        context = tokenizer.decode(tokenized_context[0], skip_special_tokens=True)\n",
        "\n",
        "    # Prompt engineering\n",
        "    # Note: For TinyLlama, a simple format works, but we add \"Answer:\" to trigger the generation.\n",
        "    prompt = (\n",
        "        f\"Use the Context below to answer the Question. \"\n",
        "        f\"If the answer is not in the Context, say 'Not enough evidence in the retrieved context.'.\\n\\n\"\n",
        "        f\"Context:\\n{context}\\n\\n\"\n",
        "        f\"Question: {question}\"\n",
        "        f\"\\n\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Generate\n",
        "    # FIXED: Increased max_new_tokens to 400 (prevents cut-offs)\n",
        "    # FIXED: Set do_sample=True (prevents the \"1.1.1.1\" repetition loop)\n",
        "    output = llm_pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=400,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        return_full_text=False\n",
        "    )\n",
        "    generated_text = output[0]['generated_text'].strip()\n",
        "\n",
        "    return (\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        f\"LLM Answer:\\n{generated_text}\"\n",
        "    )\n",
        "\n",
        "def run_query(qobj, top_k_text=TOP_K_TEXT, top_k_images=TOP_K_IMAGES, top_k_evidence=TOP_K_EVIDENCE, alpha=ALPHA) -> Dict[str, Any]:\n",
        "    question = qobj[\"question\"]\n",
        "\n",
        "    # 1. Build Context (Uses your existing function)\n",
        "    ctx = build_context(question, top_k_text=top_k_text, top_k_images=top_k_images, top_k_evidence=top_k_evidence, alpha=alpha)\n",
        "\n",
        "    # 2. Generate Answer\n",
        "    answer = llm_extractive_answer(question, ctx[\"context\"])\n",
        "\n",
        "    # 3. Return exact same structure as your original code\n",
        "    return {\n",
        "        \"id\": qobj[\"query_id\"],\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"context\": ctx[\"context\"],\n",
        "        \"image_paths\": ctx[\"image_paths\"],\n",
        "        \"text_hits\": ctx[\"text_hits\"], # Preserved\n",
        "        \"img_hits\": ctx[\"img_hits\"],   # Preserved\n",
        "    }\n",
        "\n",
        "# --- EXECUTION ---\n",
        "print(\"Running local LLM queries...\")\n",
        "results = [run_query(q) for q in mini_gold]\n",
        "\n",
        "for r in results:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(r[\"id\"], r[\"question\"])\n",
        "    print(r[\"answer\"])\n",
        "    print(\"Images:\", [os.path.basename(p) for p in r[\"image_paths\"]])"
      ],
      "metadata": {
        "id": "dDMtnovHTQnY"
      },
      "id": "dDMtnovHTQnY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Retrieval Evaluation (Precision@k / Recall@k)**\n",
        "\n",
        "We treat a text chunk as **relevant** for a query if it contains at least one `must_have_keywords` term.\n"
      ],
      "metadata": {
        "id": "FspvAaYRTy3g"
      },
      "id": "FspvAaYRTy3g"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Any, List, Set\n",
        "import pandas as pd\n",
        "\n",
        "def _normalize_evidence_id(eid: str) -> str:\n",
        "    \"\"\"Normalize evidence IDs so comparisons work.\n",
        "    Examples:\n",
        "      'img::nist_framework.png' -> 'nist_framework.png'\n",
        "      'doc1.pdf::p3' -> 'doc1.pdf'\n",
        "      'doc1.pdf' -> 'doc1.pdf'\n",
        "    \"\"\"\n",
        "    if not eid:\n",
        "        return \"\"\n",
        "    eid = eid.strip()\n",
        "    if eid.startswith(\"img::\"):\n",
        "        eid = eid.split(\"img::\", 1)[1]\n",
        "    if \"::\" in eid:\n",
        "        eid = eid.split(\"::\", 1)[0]\n",
        "    return eid\n",
        "\n",
        "def _gold_set(qobj: Dict[str, Any]) -> Set[str]:\n",
        "    return set(_normalize_evidence_id(x) for x in qobj.get(\"gold_evidence_ids\", []) if x)\n",
        "\n",
        "def _retrieved_set_from_hits(hits: List[Dict[str, Any]]) -> List[str]:\n",
        "    \"\"\"Given fused evidence dicts from build_context()['evidence'], produce normalized IDs.\"\"\"\n",
        "    out = []\n",
        "    for ev in hits:\n",
        "        if ev.get(\"modality\") == \"text\":\n",
        "            out.append(_normalize_evidence_id(ev.get(\"id\", \"\")))     # e.g., doc1.pdf::p3 -> doc1.pdf\n",
        "        else:\n",
        "            out.append(_normalize_evidence_id(ev.get(\"id\", \"\")))     # e.g., nist_framework.png\n",
        "    return out\n",
        "\n",
        "def precision_at_k(relevances: List[bool], k: int) -> float:\n",
        "    k = min(k, len(relevances))\n",
        "    if k == 0:\n",
        "        return 0.0\n",
        "    return sum(relevances[:k]) / k\n",
        "\n",
        "def recall_at_k(relevances: List[bool], k: int, total_relevant: int) -> float:\n",
        "    k = min(k, len(relevances))\n",
        "    if total_relevant == 0:\n",
        "        return 0.0\n",
        "    return sum(relevances[:k]) / total_relevant\n",
        "\n",
        "def eval_retrieval_for_query(qobj: Dict[str, Any], top_k_evidence: int = 10) -> Dict[str, Any]:\n",
        "    gold = _gold_set(qobj)\n",
        "    question = qobj[\"question\"]\n",
        "\n",
        "    # Q5-like missing-evidence case: gold is empty -> recall undefined, but we can report 0\n",
        "    # Build multimodal context and use fused evidence list\n",
        "    ctx = build_context(question, top_k_evidence=top_k_evidence)\n",
        "    retrieved_ids = _retrieved_set_from_hits(ctx[\"evidence\"])  # ordered list (ranked)\n",
        "\n",
        "    rels = [rid in gold for rid in retrieved_ids]\n",
        "    total_rel = len(gold)\n",
        "\n",
        "    return {\n",
        "        \"query_id\": qobj.get(\"query_id\", qobj.get(\"id\", \"\")),\n",
        "        \"P@5\": precision_at_k(rels, 5),\n",
        "        \"R@10\": recall_at_k(rels, 10, total_rel),\n",
        "        \"gold_count\": total_rel,\n",
        "        \"retrieved_top\": retrieved_ids[:10],\n",
        "    }\n",
        "\n",
        "eval_rows = [eval_retrieval_for_query(q) for q in mini_gold]\n",
        "df_eval = pd.DataFrame(eval_rows)\n",
        "df_eval\n"
      ],
      "metadata": {
        "id": "_Z2jZxWiT6rd"
      },
      "id": "_Z2jZxWiT6rd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "METHODS = [\"Sparse Only\", \"Dense Only\", \"Hybrid\", \"Hybrid + Rerank\", \"Multimodal\"]\n",
        "eval_results = []\n",
        "\n",
        "def normalize_evidence_id(eid: str) -> str:\n",
        "    \"\"\"Make evidence IDs comparable between gold and retrieved.\"\"\"\n",
        "    if not eid:\n",
        "        return \"\"\n",
        "    eid = eid.strip()\n",
        "    if eid.startswith(\"img::\"):\n",
        "        eid = eid.split(\"img::\", 1)[1]\n",
        "    # If chunk ids look like \"doc1.pdf::p3\", normalize to \"doc1.pdf\"\n",
        "    if \"::\" in eid:\n",
        "        eid = eid.split(\"::\", 1)[0]\n",
        "    return eid\n",
        "\n",
        "def gold_set_for_query(qobj):\n",
        "    return set(normalize_evidence_id(x) for x in qobj.get(\"gold_evidence_ids\", []) if x)\n",
        "\n",
        "def precision_at_k_bool(rels, k):\n",
        "    k = min(k, len(rels))\n",
        "    if k == 0:\n",
        "        return 0.0\n",
        "    return sum(rels[:k]) / k\n",
        "\n",
        "def recall_at_k_bool(rels, k, total_rel):\n",
        "    k = min(k, len(rels))\n",
        "    if total_rel == 0:\n",
        "        return 0.0\n",
        "    return sum(rels[:k]) / total_rel\n",
        "\n",
        "print(\"Running evaluation across all methods...\")\n",
        "\n",
        "for qobj in mini_gold:\n",
        "    qid = qobj[\"query_id\"]\n",
        "    question = qobj[\"question\"]\n",
        "    gold = gold_set_for_query(qobj)\n",
        "    total_relevant = len(gold)   # ground-truth count for this query\n",
        "\n",
        "    for method in METHODS:\n",
        "        retrieved_ids = []\n",
        "\n",
        "        if method == \"Multimodal\":\n",
        "            # Text part: use your best text method as candidates\n",
        "            text_hits = get_retrieval_results(question, \"Hybrid + Rerank\", top_k=10)\n",
        "            for idx, _ in text_hits:\n",
        "                # normalize chunk_id -> doc filename\n",
        "                retrieved_ids.append(normalize_evidence_id(page_chunks[idx].chunk_id))\n",
        "\n",
        "            # Image part\n",
        "            img_hits = tfidf_retrieve(question, img_vec, img_X, top_k=5)\n",
        "            for idx, _ in img_hits:\n",
        "                # image id should be the filename\n",
        "                retrieved_ids.append(normalize_evidence_id(image_items[idx].item_id))\n",
        "\n",
        "        else:\n",
        "            hits = get_retrieval_results(question, method, top_k=10)\n",
        "            for idx, _ in hits:\n",
        "                retrieved_ids.append(normalize_evidence_id(page_chunks[idx].chunk_id))\n",
        "\n",
        "        # Convert to relevance booleans against gold evidence ids (ranked list)\n",
        "        rels = [(rid in gold) for rid in retrieved_ids]\n",
        "\n",
        "        p5 = precision_at_k_bool(rels, 5)\n",
        "        r10 = recall_at_k_bool(rels, 10, total_relevant)\n",
        "\n",
        "        eval_results.append({\n",
        "            \"Query\": qid,\n",
        "            \"Method\": method,\n",
        "            \"Precision@5\": round(p5, 2),\n",
        "            \"Recall@10\": round(r10, 2),\n",
        "            \"Gold_Evidence_Count\": total_relevant,\n",
        "            \"Gold_Evidence\": sorted(list(gold)),\n",
        "            \"Retrieved_Top\": retrieved_ids[:10],\n",
        "        })\n",
        "\n",
        "df_results = pd.DataFrame(eval_results)\n",
        "\n",
        "print(\"\\n=== Final Deliverable Table (Query x Method x Metrics) ===\")\n",
        "display(df_results[[\"Query\",\"Method\",\"Precision@5\",\"Recall@10\",\"Gold_Evidence_Count\"]])\n",
        "\n",
        "print(\"\\n=== Comparison View (Precision@5) ===\")\n",
        "display(df_results.pivot(index=\"Query\", columns=\"Method\", values=\"Precision@5\"))\n",
        "\n",
        "print(\"\\n=== Comparison View (Recall@10) ===\")\n",
        "display(df_results.pivot(index=\"Query\", columns=\"Method\", values=\"Recall@10\"))\n"
      ],
      "metadata": {
        "id": "fdDXxmK4VVDF"
      },
      "id": "fdDXxmK4VVDF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d7629d00",
      "metadata": {
        "id": "d7629d00"
      },
      "source": [
        "# 6) Evaluation + Logging (Required)\n",
        "\n",
        "Every query must append to: `logs/query_metrics.csv`\n",
        "\n",
        "Required columns (minimum):\n",
        "- timestamp\n",
        "- query_id\n",
        "- retrieval_mode\n",
        "- top_k\n",
        "- latency_ms\n",
        "- Precision@5\n",
        "- Recall@10\n",
        "- evidence_ids_returned\n",
        "- faithfulness_pass\n",
        "- missing_evidence_behavior\n",
        "\n",
        "> If your gold set is incomplete (common for Q4/Q5), compute P/R only for labeled queries and still log latency/evidence IDs.\n",
        "\n",
        "## How we define metrics (simple)\n",
        "- `Precision@K`: (# retrieved evidence IDs in gold) / K\n",
        "- `Recall@K`: (# retrieved evidence IDs in gold) / (size of gold set)\n",
        "\n",
        "**Faithfulness (Yes/No):**\n",
        "- Yes if the answer **only** uses retrieved evidence and includes citations.\n",
        "- For this template, we implement a simple heuristic. Replace with your rubric/judge if desired.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850487f0",
      "metadata": {
        "id": "850487f0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "\n",
        "\n",
        "def _canon_evidence_id(x: str) -> str:\n",
        "    x = str(x).strip()\n",
        "    # keep img:: prefix intact\n",
        "    if x.startswith('img::'):\n",
        "        return x\n",
        "    # normalize file ids: allow with/without extension\n",
        "    if x.endswith('.txt'):\n",
        "        return x[:-4]\n",
        "    return x\n",
        "\n",
        "def _normalize_retrieved_ids(retrieved):\n",
        "    \"\"\"Normalize retrieved outputs into a list of evidence IDs.\n",
        "    Returns canonical IDs (doc_id without .txt, or img::filename).\n",
        "\n",
        "    Supports: list[dict], list[(idx,score)], list[str].\n",
        "    \"\"\"\n",
        "    if retrieved is None:\n",
        "        return []\n",
        "    if len(retrieved) == 0:\n",
        "        return []\n",
        "    # list[str]\n",
        "    if isinstance(retrieved[0], str):\n",
        "        return [_canon_evidence_id(r) for r in retrieved]\n",
        "    # list[dict]\n",
        "    if isinstance(retrieved[0], dict):\n",
        "        out=[]\n",
        "        for r in retrieved:\n",
        "            if 'evidence_id' in r and r['evidence_id']:\n",
        "                out.append(_canon_evidence_id(r['evidence_id']))\n",
        "            elif 'doc_id' in r and r['doc_id']:\n",
        "                out.append(_canon_evidence_id(r['doc_id']))\n",
        "            elif 'source' in r and r['source']:\n",
        "                out.append(_canon_evidence_id(os.path.basename(str(r['source']))))\n",
        "        return out\n",
        "    # list[(idx, score)]\n",
        "    if isinstance(retrieved[0], (tuple, list)) and len(retrieved[0]) >= 1:\n",
        "        out=[]\n",
        "        for item in retrieved:\n",
        "            idx = int(item[0])\n",
        "            if 'items' in globals() and 0 <= idx < len(items):\n",
        "                out.append(_canon_evidence_id(items[idx].get('evidence_id')))\n",
        "            elif 'documents' in globals() and 0 <= idx < len(documents):\n",
        "                out.append(_canon_evidence_id(documents[idx].get('doc_id') or os.path.basename(documents[idx].get('source',''))))\n",
        "        return out\n",
        "    return []\n",
        "\n",
        "def _normalize_gold_ids(gold_ids):\n",
        "    if not gold_ids or gold_ids == ['N/A']:\n",
        "        return None\n",
        "    return [_canon_evidence_id(g) for g in gold_ids]\n",
        "\n",
        "def precision_at_k(retrieved, gold_ids, k):\n",
        "    gold = _normalize_gold_ids(gold_ids)\n",
        "    if gold is None:\n",
        "        return None\n",
        "    retrieved_ids = _normalize_retrieved_ids(retrieved)[:k]\n",
        "    if k == 0:\n",
        "        return None\n",
        "    return len(set(retrieved_ids) & set(gold)) / float(k)\n",
        "\n",
        "def recall_at_k(retrieved, gold_ids, k):\n",
        "    gold = _normalize_gold_ids(gold_ids)\n",
        "    if gold is None:\n",
        "        return None\n",
        "    retrieved_ids = _normalize_retrieved_ids(retrieved)[:k]\n",
        "    denom = float(len(set(gold)))\n",
        "    return (len(set(retrieved_ids) & set(gold)) / denom) if denom > 0 else None\n",
        "\n",
        "\n",
        "\n",
        "def faithfulness_heuristic(answer: str, evidence: list):\n",
        "    # Simple heuristic: answer includes at least one citation tag from evidence OR is missing-evidence msg\n",
        "    if answer.strip() == MISSING_EVIDENCE_MSG:\n",
        "        return True\n",
        "    tags = [e[\"citation_tag\"] for e in evidence[:5]]\n",
        "    return any(tag in answer for tag in tags)\n",
        "\n",
        "def missing_evidence_behavior(answer: str, evidence: list):\n",
        "    # Pass if either: evidence present and answer not missing-evidence; or evidence absent and answer is missing-evidence msg\n",
        "    has_ev = bool(evidence) and max(e.get(\"score\", 0.0) for e in evidence) >= 0.05\n",
        "    if not has_ev:\n",
        "        return \"Pass\" if answer.strip() == MISSING_EVIDENCE_MSG else \"Fail\"\n",
        "    else:\n",
        "        return \"Pass\" if answer.strip() != MISSING_EVIDENCE_MSG else \"Fail\"\n",
        "\n",
        "def ensure_logfile(path: str, header: list):\n",
        "    p = Path(path)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if not p.exists():\n",
        "        with open(p, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(header)\n",
        "\n",
        "LOG_HEADER = [\n",
        "    \"timestamp\", \"query_id\", \"retrieval_mode\", \"top_k\", \"latency_ms\",\n",
        "    \"Precision@5\", \"Recall@10\",\n",
        "    \"evidence_ids_returned\", \"gold_evidence_ids\",\n",
        "    \"faithfulness_pass\", \"missing_evidence_behavior\"\n",
        "]\n",
        "ensure_logfile(cfg.log_file, LOG_HEADER)\n",
        "\n",
        "def run_query_and_log(query_item, retrieval_mode = 'hybrid', top_k=10):\n",
        "    question = query_item[\"question\"]\n",
        "    gold_ids = query_item.get(\"gold_evidence_ids\", [])\n",
        "\n",
        "    t0 = time.time()\n",
        "    evidence = retrieve_tfidf(question, top_k=top_k)  # replace with your pipeline + modes\n",
        "    answer = generate_answer_stub(question, evidence) # replace with LLM/VLM\n",
        "    latency_ms = (time.time() - t0) * 1000.0\n",
        "\n",
        "    retrieved_ids = [e[\"chunk_id\"] for e in evidence]\n",
        "    p5 = precision_at_k(retrieved_ids, gold_ids, cfg.eval_p_at) if gold_ids else np.nan\n",
        "    r10 = recall_at_k(retrieved_ids, gold_ids, cfg.eval_r_at) if gold_ids else np.nan\n",
        "\n",
        "    faithful = faithfulness_heuristic(answer, evidence)\n",
        "    meb = missing_evidence_behavior(answer, evidence)\n",
        "\n",
        "    row = [\n",
        "        datetime.now(timezone.utc).isoformat(),\n",
        "        query_item[\"query_id\"],\n",
        "        retrieval_mode,\n",
        "        top_k,\n",
        "        round(latency_ms, 2),\n",
        "        p5,\n",
        "        r10,\n",
        "        json.dumps(retrieved_ids),\n",
        "        json.dumps(gold_ids),\n",
        "        \"Yes\" if faithful else \"No\",\n",
        "        meb\n",
        "    ]\n",
        "    with open(cfg.log_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(row)\n",
        "\n",
        "    return {\"answer\": answer, \"evidence\": evidence, \"p5\": p5, \"r10\": r10, \"latency_ms\": latency_ms, \"faithful\": faithful, \"meb\": meb}\n",
        "\n",
        "# Run all five queries once (demo)\n",
        "results = []\n",
        "for qi in mini_gold:\n",
        "    results.append(run_query_and_log(qi, retrieval_mode = 'hybrid', top_k=cfg.top_k_default))\n",
        "\n",
        "pd.read_csv(cfg.log_file).tail(8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ungUxBVhbK44",
      "metadata": {
        "id": "ungUxBVhbK44"
      },
      "outputs": [],
      "source": [
        "# Task: Run retrieval + answer generation for all mini-gold queries\n",
        "# This cell is self-contained: if retrieval/indexing cells were skipped, it will bootstrap a TF-IDF retriever.\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Build a local evidence list if not already present\n",
        "if 'items' in globals():\n",
        "    _evidence = items\n",
        "elif 'documents' in globals():\n",
        "    _evidence = []\n",
        "    for d in documents:\n",
        "        _evidence.append({\n",
        "            'evidence_id': d.get('doc_id') or os.path.basename(d.get('source','')),\n",
        "            'modality': 'text',\n",
        "            'source': d.get('source'),\n",
        "            'text': d.get('text','')\n",
        "        })\n",
        "else:\n",
        "    raise NameError('Neither items nor documents are defined. Run the ZIP extraction + document loading cells first.')\n",
        "\n",
        "assert len(_evidence) > 0, 'Evidence store is empty.'\n",
        "\n",
        "# Canonicalize evidence ids for consistent evaluation\n",
        "def _canon_evidence_id(x: str) -> str:\n",
        "    x = str(x).strip()\n",
        "    if x.startswith('img::'):\n",
        "        return x\n",
        "    return x[:-4] if x.endswith('.txt') else x\n",
        "\n",
        "# Bootstrap TF-IDF retriever if no retriever exists\n",
        "if 'retrieve_hybrid' not in globals() and 'retrieve_tfidf' not in globals() and 'retrieve' not in globals():\n",
        "    _texts = [it.get('text','') for it in _evidence]\n",
        "    _tfidf = TfidfVectorizer(stop_words=None, token_pattern=r'(?u)\\b\\w+\\b')\n",
        "    _tfidf_mat = _tfidf.fit_transform(_texts)\n",
        "\n",
        "    def retrieve_tfidf(query, top_k=10):\n",
        "        qv = _tfidf.transform([query])\n",
        "        sims = cosine_similarity(qv, _tfidf_mat).ravel()\n",
        "        idx = np.argsort(sims)[::-1][:top_k]\n",
        "        return [(int(i), float(sims[i])) for i in idx]\n",
        "\n",
        "# Define retrieve() wrapper if missing\n",
        "if 'retrieve' not in globals():\n",
        "    def retrieve(question, retrieval_mode='hybrid', top_k=10, alpha=0.6):\n",
        "        # Prefer hybrid if available; otherwise TF-IDF\n",
        "        if retrieval_mode == 'hybrid' and 'retrieve_hybrid' in globals():\n",
        "            hits = retrieve_hybrid(question, top_k=top_k, alpha=alpha)\n",
        "            return hits, {'mode':'hybrid'}\n",
        "        if 'retrieve_tfidf' in globals():\n",
        "            hits = retrieve_tfidf(question, top_k=top_k)\n",
        "            return hits, {'mode':'tfidf'}\n",
        "        raise NameError('No retriever available. Execute the retrieval/indexing section.')\n",
        "\n",
        "# Ensure build_context exists\n",
        "if 'build_context' not in globals():\n",
        "    def build_context(hit_ids, max_chars=1400):\n",
        "        parts=[]\n",
        "        for i in hit_ids:\n",
        "            parts.append(f\"[{_evidence[i].get('evidence_id')}] {_evidence[i].get('text','')}\")\n",
        "        ctx='\\n'.join(parts)\n",
        "        return ctx[:max_chars]\n",
        "\n",
        "# Ensure extractive_answer exists\n",
        "if 'extractive_answer' not in globals():\n",
        "    import re\n",
        "    def extractive_answer(query, context):\n",
        "        q=set(re.findall(r'[A-Za-z]+', query.lower()))\n",
        "        sents=re.split(r'(?<=[.!?])\\s+', (context or '').strip())\n",
        "        scored=[]\n",
        "        for s in sents:\n",
        "            w=set(re.findall(r'[A-Za-z]+', s.lower()))\n",
        "            scored.append((len(q & w), s.strip()))\n",
        "        scored.sort(key=lambda x:x[0], reverse=True)\n",
        "        best=[s for sc,s in scored[:3] if sc>0]\n",
        "        return ' '.join(best) if best else 'Not enough information in the context.'\n",
        "\n",
        "rows=[]\n",
        "for ex in mini_gold:\n",
        "    qid = ex.get('query_id')\n",
        "    question = ex.get('question')\n",
        "    gold = ex.get('gold_evidence_ids')\n",
        "\n",
        "    if 'run_query_and_log' in globals():\n",
        "        # Call run_query_and_log with the full query item dictionary 'ex'\n",
        "        out = run_query_and_log(ex, retrieval_mode='hybrid', top_k=10)\n",
        "        answer = out.get('answer')\n",
        "        # The 'evidence' key from run_query_and_log output contains a list of dicts with 'chunk_id'\n",
        "        evidence = [e['chunk_id'] for e in out.get('evidence', [])]\n",
        "    else:\n",
        "        hits, debug = retrieve(question, retrieval_mode='hybrid', top_k=10)\n",
        "        hit_ids = [int(i) for i,_ in hits]\n",
        "        context = build_context(hit_ids[:10])\n",
        "        answer = extractive_answer(question, context)\n",
        "        evidence = [_canon_evidence_id(_evidence[i].get('evidence_id')) for i in hit_ids[:10]]\n",
        "\n",
        "    rows.append({\n",
        "        'query_id': qid,\n",
        "        'question': question,\n",
        "        'answer': answer,\n",
        "        'evidence_ids_returned(top10)': evidence,\n",
        "        'gold_evidence_ids': gold,\n",
        "    })\n",
        "\n",
        "df_answers = pd.DataFrame(rows)\n",
        "df_answers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e71b22",
      "metadata": {
        "id": "46e71b22"
      },
      "source": [
        "# 7) Streamlit App Skeleton (Required)\n",
        "\n",
        "You will create a Streamlit app file in your repo, e.g.:\n",
        "\n",
        "- `app/main.py`\n",
        "\n",
        "This notebook can generate a starter `app/main.py` for your team.\n",
        "\n",
        "### Required UI components\n",
        "- Query input box\n",
        "- Retrieval controls (mode, top_k, multimodal toggle if applicable)\n",
        "- Answer panel\n",
        "- Evidence panel (with citations)\n",
        "- Metrics panel (latency, P@5, R@10 if available)\n",
        "- Logging happens automatically on each query\n",
        "\n",
        "> This skeleton calls functions in your Python modules. Prefer moving retrieval logic into `/rag/` and importing it.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n"
      ],
      "metadata": {
        "id": "JFzXkEthOxrq"
      },
      "id": "JFzXkEthOxrq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb966c49",
      "metadata": {
        "id": "bb966c49"
      },
      "outputs": [],
      "source": [
        "# Generate a starter Streamlit app file (edit paths as needed).\n",
        "# In your repo: create /app/main.py and move shared logic into /rag/\n",
        "\n",
        "streamlit_code = r'''\n",
        "import json, time\n",
        "from pathlib import Path\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "# --- Import your team pipeline here ---\n",
        "# from rag.pipeline import retrieve, generate_answer, run_query_and_log\n",
        "\n",
        "MISSING_EVIDENCE_MSG = \"Not enough evidence in the retrieved context.\"\n",
        "\n",
        "st.set_page_config(page_title=\"CS5542 Lab 4 ‚Äî Project RAG App\", layout=\"wide\")\n",
        "st.title(\"CS 5542 Lab 4 ‚Äî Project RAG Application\")\n",
        "st.caption(\"Project-aligned Streamlit UI + automatic logging + failure monitoring\")\n",
        "\n",
        "# Sidebar controls\n",
        "st.sidebar.header(\"Retrieval Settings\")\n",
        "retrieval_mode = st.sidebar.selectbox(\"retrieval_mode\", [\"tfidf\", \"dense\", \"sparse\", \"hybrid\", \"hybrid_rerank\"])\n",
        "top_k = st.sidebar.slider(\"top_k\", min_value=1, max_value=30, value=10, step=1)\n",
        "use_multimodal = st.sidebar.checkbox(\"use_multimodal\", value=True)\n",
        "\n",
        "st.sidebar.header(\"Logging\")\n",
        "log_path = st.sidebar.text_input(\"log file\", value=\"logs/query_metrics.csv\")\n",
        "\n",
        "# --- Mini gold set (replace with your team's Q1‚ÄìQ5) ---\n",
        "# Tip: keep the same structure as in your Lab 4 notebook so IDs match logs.\n",
        "MINI_GOLD = {\n",
        "    \"Q1\": {\"question\": \"Replace with your project Q1\", \"gold_evidence_ids\": []},\n",
        "    \"Q2\": {\"question\": \"Replace with your project Q2\", \"gold_evidence_ids\": []},\n",
        "    \"Q3\": {\"question\": \"Replace with your project Q3\", \"gold_evidence_ids\": []},\n",
        "    \"Q4\": {\"question\": \"Replace with your project Q4 (multimodal/table/figure)\", \"gold_evidence_ids\": []},\n",
        "    \"Q5\": {\"question\": \"Replace with your project Q5 (missing-evidence case)\", \"gold_evidence_ids\": []},\n",
        "}\n",
        "\n",
        "st.sidebar.header(\"Evaluation\")\n",
        "query_id = st.sidebar.selectbox(\"query_id (for logging)\", list(MINI_GOLD.keys()))\n",
        "use_gold_question = st.sidebar.checkbox(\"Use the gold-set question text\", value=True)\n",
        "\n",
        "# Main query\n",
        "default_q = MINI_GOLD[query_id][\"question\"] if use_gold_question else \"\"\n",
        "question = st.text_area(\"Enter your question\", value=default_q, height=120)\n",
        "run_btn = st.button(\"Run Query\")\n",
        "\n",
        "colA, colB = st.columns([2, 1])\n",
        "\n",
        "def ensure_logfile(path: str):\n",
        "    p = Path(path)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if not p.exists():\n",
        "        df = pd.DataFrame(columns=[\n",
        "            \"timestamp\",\"query_id\",\"retrieval_mode\",\"top_k\",\"latency_ms\",\n",
        "            \"Precision@5\",\"Recall@10\",\"evidence_ids_returned\",\"gold_evidence_ids\",\n",
        "            \"faithfulness_pass\",\"missing_evidence_behavior\"\n",
        "        ])\n",
        "        df.to_csv(p, index=False)\n",
        "\n",
        "def precision_at_k(retrieved_ids, gold_ids, k=5):\n",
        "    if not gold_ids:\n",
        "        return None\n",
        "    topk = retrieved_ids[:k]\n",
        "    hits = sum(1 for x in topk if x in set(gold_ids))\n",
        "    return hits / k\n",
        "\n",
        "def recall_at_k(retrieved_ids, gold_ids, k=10):\n",
        "    if not gold_ids:\n",
        "        return None\n",
        "    topk = retrieved_ids[:k]\n",
        "    hits = sum(1 for x in topk if x in set(gold_ids))\n",
        "    return hits / max(1, len(gold_ids))\n",
        "\n",
        "# ---- Placeholder demo logic (replace with imports from your /rag module) ----\n",
        "def retrieve_demo(q: str, top_k: int):\n",
        "    return [{\"chunk_id\":\"demo_doc\",\"citation_tag\":\"[demo_doc]\",\"score\":0.9,\"source\":\"data/docs/demo_doc.txt\",\"text\":\"demo evidence...\"}]\n",
        "\n",
        "def answer_demo(q: str, evidence: list):\n",
        "    if not evidence:\n",
        "        return MISSING_EVIDENCE_MSG\n",
        "    return f\"Grounded answer using {evidence[0]['citation_tag']} {evidence[0]['citation_tag']}\"\n",
        "\n",
        "def log_row(path: str, row: dict):\n",
        "    ensure_logfile(path)\n",
        "    df = pd.read_csv(path)\n",
        "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
        "    df.to_csv(path, index=False)\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "if run_btn and question.strip():\n",
        "    t0 = time.time()\n",
        "    evidence = retrieve_demo(question, top_k=top_k)\n",
        "    answer = answer_demo(question, evidence)\n",
        "    latency_ms = round((time.time() - t0)*1000, 2)\n",
        "\n",
        "    retrieved_ids = [e[\"chunk_id\"] for e in evidence]\n",
        "    gold_ids = MINI_GOLD[query_id].get(\"gold_evidence_ids\", [])\n",
        "\n",
        "    p5 = precision_at_k(retrieved_ids, gold_ids, k=5)\n",
        "    r10 = recall_at_k(retrieved_ids, gold_ids, k=10)\n",
        "\n",
        "    with colA:\n",
        "        st.subheader(\"Answer\")\n",
        "        st.write(answer)\n",
        "\n",
        "        st.subheader(\"Evidence (Top-K)\")\n",
        "        st.json(evidence)\n",
        "\n",
        "    with colB:\n",
        "        st.subheader(\"Metrics\")\n",
        "        st.write({\"latency_ms\": latency_ms, \"Precision@5\": p5, \"Recall@10\": r10})\n",
        "\n",
        "    # Log the query using the selected Q1‚ÄìQ5 ID (not ad-hoc)\n",
        "    row = {\n",
        "        \"timestamp\": pd.Timestamp.utcnow().isoformat(),\n",
        "        \"query_id\": query_id,\n",
        "        \"retrieval_mode\": retrieval_mode,\n",
        "        \"top_k\": top_k,\n",
        "        \"latency_ms\": latency_ms,\n",
        "        \"Precision@5\": p5,\n",
        "        \"Recall@10\": r10,\n",
        "        \"evidence_ids_returned\": json.dumps(retrieved_ids),\n",
        "        \"gold_evidence_ids\": json.dumps(gold_ids),\n",
        "        \"faithfulness_pass\": \"Yes\" if answer != MISSING_EVIDENCE_MSG else \"Yes\",\n",
        "        \"missing_evidence_behavior\": \"Pass\"  # update with your rule if needed\n",
        "    }\n",
        "    log_row(log_path, row)\n",
        "    st.success(f\"Logged {query_id} to CSV.\")\n",
        "'''\n",
        "app_dir = Path(\"app\")\n",
        "app_dir.mkdir(parents=True, exist_ok=True)\n",
        "(app_dir / \"main.py\").write_text(streamlit_code, encoding=\"utf-8\")\n",
        "print(\"Wrote starter Streamlit app to:\", app_dir / \"main.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c21ec8",
      "metadata": {
        "id": "05c21ec8"
      },
      "source": [
        "# 8) Optional Extension ‚Äî FastAPI Backend (Recommended for larger teams)\n",
        "\n",
        "If your team selects the **FastAPI extension**, create:\n",
        "- `api/server.py` with `POST /query`\n",
        "- Streamlit UI calls the API using `requests.post(...)`\n",
        "\n",
        "This separation mirrors real production systems:\n",
        "UI (Streamlit) ‚Üí API (FastAPI) ‚Üí Retrieval + LLM services\n",
        "\n",
        "Below is a minimal FastAPI starter you can generate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32168ff2",
      "metadata": {
        "id": "32168ff2"
      },
      "outputs": [],
      "source": [
        "fastapi_code = r'''\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "app = FastAPI(title=\"CS5542 Lab 4 RAG Backend\")\n",
        "\n",
        "MISSING_EVIDENCE_MSG = \"Not enough evidence in the retrieved context.\"\n",
        "\n",
        "class QueryIn(BaseModel):\n",
        "    question: str\n",
        "    top_k: int = 10\n",
        "    retrieval_mode: str = \"hybrid\"\n",
        "    use_multimodal: bool = True\n",
        "\n",
        "@app.post(\"/query\")\n",
        "def query(q: QueryIn) -> Dict[str, Any]:\n",
        "    # TODO: import your real pipeline:\n",
        "    # evidence = retrieve(q.question, top_k=q.top_k, mode=q.retrieval_mode, use_multimodal=q.use_multimodal)\n",
        "    # answer = generate_answer(q.question, evidence)\n",
        "    evidence = [{\"chunk_id\":\"demo_doc\",\"citation_tag\":\"[demo_doc]\",\"score\":0.9,\"source\":\"data/docs/demo_doc.txt\",\"text\":\"demo evidence...\"}]\n",
        "    answer = f\"Grounded answer using {evidence[0]['citation_tag']} {evidence[0]['citation_tag']}\"\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"evidence\": evidence,\n",
        "        \"metrics\": {\"top_k\": q.top_k, \"retrieval_mode\": q.retrieval_mode},\n",
        "        \"failure_flag\": False\n",
        "    }\n",
        "'''\n",
        "api_dir = Path(\"api\")\n",
        "api_dir.mkdir(parents=True, exist_ok=True)\n",
        "(api_dir / \"server.py\").write_text(fastapi_code, encoding=\"utf-8\")\n",
        "print(\"Wrote starter FastAPI server to:\", api_dir / \"server.py\")\n",
        "\n",
        "print(\"\\nRun locally (terminal):\")\n",
        "print(\"  uvicorn api.server:app --reload --port 8000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Temporary Build\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# We embed the pipeline logic (loading, indexing, retrieval) directly into the server file\n",
        "# so it runs independently of the notebook kernel.\n",
        "fastapi_code = r'''\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "app = FastAPI(title=\"CS5542 Lab 4 RAG Backend\")\n",
        "\n",
        "# --- 1. Global State & Data Loading ---\n",
        "# We store the index and evidence globally so they load once on startup\n",
        "global_state = {\n",
        "    \"vectorizer\": None,\n",
        "    \"tfidf_matrix\": None,\n",
        "    \"evidence\": []\n",
        "}\n",
        "\n",
        "def load_data_and_index():\n",
        "    \"\"\"Loads text files from ./data/docs and builds a TF-IDF index.\"\"\"\n",
        "    print(\"Loading data from ./data/docs...\")\n",
        "    docs_dir = \"./data/docs\"\n",
        "\n",
        "    # Simple loader matching your notebook's logic\n",
        "    items = []\n",
        "    if os.path.exists(docs_dir):\n",
        "        files = glob.glob(os.path.join(docs_dir, \"*.txt\")) + glob.glob(os.path.join(docs_dir, \"*.pdf\"))\n",
        "        for p in files:\n",
        "            # For simplicity in this demo server, we read text files directly.\n",
        "            # If using PDFs, you'd include PyMuPDF logic here or assume pre-converted .txts exist.\n",
        "            try:\n",
        "                # Fallback: try reading as text (works for the .txt demo files)\n",
        "                with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    text = f.read()\n",
        "\n",
        "                items.append({\n",
        "                    \"evidence_id\": os.path.basename(p),\n",
        "                    \"source\": p,\n",
        "                    \"text\": text,\n",
        "                    \"citation_tag\": f\"[{os.path.basename(p)}]\"\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping file {p}: {e}\")\n",
        "\n",
        "    if not items:\n",
        "        print(\"WARNING: No documents found in ./data/docs. Server will have empty index.\")\n",
        "        return\n",
        "\n",
        "    # Build TF-IDF Index\n",
        "    texts = [it[\"text\"] for it in items]\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "    global_state[\"evidence\"] = items\n",
        "    global_state[\"vectorizer\"] = vectorizer\n",
        "    global_state[\"tfidf_matrix\"] = tfidf_matrix\n",
        "    print(f\"Server ready: Indexed {len(items)} documents.\")\n",
        "\n",
        "# Load on startup\n",
        "@app.on_event(\"startup\")\n",
        "def startup_event():\n",
        "    load_data_and_index()\n",
        "\n",
        "# --- 2. Your Pipeline Functions (Ported from Notebook) ---\n",
        "\n",
        "def retrieve_tfidf(question: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
        "    vec = global_state[\"vectorizer\"]\n",
        "    mat = global_state[\"tfidf_matrix\"]\n",
        "\n",
        "    if vec is None or mat is None:\n",
        "        return []\n",
        "\n",
        "    q_vec = vec.transform([question])\n",
        "    sims = cosine_similarity(q_vec, mat).ravel()\n",
        "    # Get top_k indices\n",
        "    idxs = np.argsort(-sims)[:top_k]\n",
        "    return [(int(i), float(sims[i])) for i in idxs]\n",
        "\n",
        "def build_context(hit_ids: List[int], max_chars=2000) -> str:\n",
        "    parts = []\n",
        "    current_len = 0\n",
        "    for i in hit_ids:\n",
        "        item = global_state[\"evidence\"][i]\n",
        "        text = item[\"text\"]\n",
        "        tag = item[\"citation_tag\"]\n",
        "        # Simple truncation for context window\n",
        "        entry = f\"{tag} {text}\"\n",
        "        if current_len + len(entry) > max_chars:\n",
        "            break\n",
        "        parts.append(entry)\n",
        "        current_len += len(entry)\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "def extractive_answer(query: str, context: str) -> str:\n",
        "    \"\"\"Simple heuristic answer generator from your notebook.\"\"\"\n",
        "    if not context.strip():\n",
        "        return \"Not enough evidence in the retrieved context.\"\n",
        "\n",
        "    # Heuristic: Find sentences with overlapping words\n",
        "    q_words = set(re.findall(r'[A-Za-z]+', query.lower()))\n",
        "    # Split by simple punctuation\n",
        "    sents = re.split(r'(?<=[.!?])\\s+', context)\n",
        "\n",
        "    scored = []\n",
        "    for s in sents:\n",
        "        w = set(re.findall(r'[A-Za-z]+', s.lower()))\n",
        "        score = len(q_words & w)\n",
        "        if score > 0:\n",
        "            scored.append((score, s.strip()))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # Return top 3 sentences or fallback\n",
        "    best = [s for sc, s in scored[:3]]\n",
        "    if best:\n",
        "        return \" \".join(best)\n",
        "    return \"Not enough evidence in the retrieved context.\"\n",
        "\n",
        "# --- 3. API Endpoint ---\n",
        "\n",
        "class QueryIn(BaseModel):\n",
        "    question: str\n",
        "    top_k: int = 5\n",
        "    retrieval_mode: str = \"hybrid\" # We use tfidf fallback in this server\n",
        "\n",
        "@app.post(\"/query\")\n",
        "def query(q: QueryIn) -> Dict[str, Any]:\n",
        "    # 1. Retrieve\n",
        "    # (Currently forcing TF-IDF pipeline for the server demo)\n",
        "    hits = retrieve_tfidf(q.question, top_k=q.top_k)\n",
        "\n",
        "    # 2. Format Evidence\n",
        "    evidence_list = []\n",
        "    hit_indices = []\n",
        "    for idx, score in hits:\n",
        "        item = global_state[\"evidence\"][idx]\n",
        "        evidence_list.append({\n",
        "            \"chunk_id\": item[\"evidence_id\"],\n",
        "            \"citation_tag\": item[\"citation_tag\"],\n",
        "            \"score\": score,\n",
        "            \"source\": item[\"source\"],\n",
        "            \"text\": item[\"text\"][:500] + \"...\" # Truncate for API response payload\n",
        "        })\n",
        "        hit_indices.append(idx)\n",
        "\n",
        "    # 3. Generate Answer\n",
        "    context = build_context(hit_indices)\n",
        "    answer = extractive_answer(q.question, context)\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"evidence\": evidence_list,\n",
        "        \"metrics\": {\n",
        "            \"top_k\": q.top_k,\n",
        "            \"retrieval_mode\": \"tfidf_server_baseline\"\n",
        "        },\n",
        "        \"failure_flag\": answer == \"Not enough evidence in the retrieved context.\"\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    # Allow running directly via python api/server.py\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "'''\n",
        "\n",
        "api_dir = Path(\"api\")\n",
        "api_dir.mkdir(parents=True, exist_ok=True)\n",
        "(api_dir / \"server.py\").write_text(fastapi_code, encoding=\"utf-8\")\n",
        "print(\"‚úÖ Wrote self-contained FastAPI server to:\", api_dir / \"server.py\")\n",
        "print(\"\\nRun locally (terminal):\")\n",
        "print(\"  uvicorn api.server:app --reload --port 8000\")"
      ],
      "metadata": {
        "id": "eqMRoTBcPGm6"
      },
      "id": "eqMRoTBcPGm6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9351032a",
      "metadata": {
        "id": "9351032a"
      },
      "source": [
        "# 9) Deployment checklist (Required)\n",
        "\n",
        "Choose **one** deployment route and publish the public link in your README:\n",
        "\n",
        "- HuggingFace Spaces (Streamlit)\n",
        "- Streamlit Cloud (GitHub-connected)\n",
        "- Render / Railway (GitHub-connected)\n",
        "\n",
        "## README must include\n",
        "1. Public deployment link  \n",
        "2. How to run locally:\n",
        "   - `pip install -r requirements.txt`\n",
        "   - `streamlit run app/main.py`\n",
        "3. A screenshot of:\n",
        "   - the UI\n",
        "   - evidence panel\n",
        "   - metrics panel\n",
        "4. Results snapshot:\n",
        "   - **5 queries √ó 2 retrieval modes**\n",
        "5. Failure analysis:\n",
        "   - 2 failure cases, root cause, proposed fix\n",
        "\n",
        "---\n",
        "\n",
        "# 10) Failure analysis template (Required)\n",
        "\n",
        "Document:\n",
        "1. **Retrieval failure** (wrong evidence or missed gold evidence)  \n",
        "2. **Grounding / missing-evidence failure** (safe behavior or citation enforcement)\n",
        "\n",
        "For each:\n",
        "- What happened?\n",
        "- Why did it happen (root cause)?\n",
        "- What change will you implement next?\n",
        "\n",
        "You can paste your analysis into your README under **Lab 4 Results**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67c11a7f",
      "metadata": {
        "id": "67c11a7f"
      },
      "source": [
        "# 11) Team checklist (quick)\n",
        "\n",
        "Before submission, verify:\n",
        "\n",
        "- [ ] Dataset, UI, and models are **project-aligned**\n",
        "- [ ] Streamlit app runs locally and shows: answer + evidence + metrics\n",
        "- [ ] `logs/query_metrics.csv` is auto-created and appended per query\n",
        "- [ ] Mini gold set Q1‚ÄìQ5 exists and P@5/R@10 computed when possible\n",
        "- [ ] Deployed link is public and listed in README\n",
        "- [ ] Two failure cases documented with fixes\n",
        "- [ ] `requirements.txt` and run instructions are correct\n",
        "- [ ] Individual survey submitted by each teammate\n",
        "\n",
        "---\n",
        "\n",
        "If you want to go beyond: add an evaluation dashboard, reranking integration, or FastAPI separation (extensions).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4l9AWSmiSN26",
      "metadata": {
        "id": "4l9AWSmiSN26"
      },
      "outputs": [],
      "source": [
        "# Verification: retrieval should return non-empty results for a project-relevant query\n",
        "\n",
        "test_q = \"State Data Breach Notification Laws HIPAA GLBA\"\n",
        "\n",
        "try:\n",
        "    if 'retrieve_tfidf' in globals():\n",
        "        hits = retrieve_tfidf(test_q, top_k=5)\n",
        "    elif 'retrieve' in globals():\n",
        "        hits = retrieve(test_q, top_k=5)\n",
        "    else:\n",
        "        hits = []\n",
        "\n",
        "    if hits is None:\n",
        "        hits = []\n",
        "\n",
        "    n = len(hits) if hasattr(hits, '__len__') else 0\n",
        "    print('Project retrieval hits:', n)\n",
        "    assert n > 0, 'Retrieval returned empty results. Check corpus + indexing.'\n",
        "\n",
        "    # Show top hit preview so you know it matches your dataset\n",
        "    i0, s0 = hits[0]\n",
        "    print(\"Top hit:\", getattr(page_chunks[i0], \"chunk_id\", f\"chunk_{i0}\"), \"score=\", s0)\n",
        "    print(\"Preview:\", (page_chunks[i0].text or \"\")[:200].replace(\"\\n\", \" \"))\n",
        "\n",
        "except Exception as e:\n",
        "    print('‚ö†Ô∏è Retrieval verification could not run.')\n",
        "    print('Reason:', type(e).__name__, str(e)[:180])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71062ba0",
      "metadata": {
        "id": "71062ba0"
      },
      "source": [
        "\n",
        "## GitHub Deployment Example\n",
        "\n",
        "### Step 1 ‚Äî Push to GitHub\n",
        "```bash\n",
        "git init\n",
        "git add .\n",
        "git commit -m \"Lab4 deployment\"\n",
        "git branch -M main\n",
        "git remote add origin https://github.com/<username>/<repo>.git\n",
        "git push -u origin main\n",
        "```\n",
        "\n",
        "### Step 2 ‚Äî Deploy using Streamlit Cloud\n",
        "1. Visit https://share.streamlit.io\n",
        "2. Click **New App**\n",
        "3. Select your GitHub repository\n",
        "4. Branch: `main`\n",
        "5. App path: `app/main.py`\n",
        "6. Click **Deploy**\n",
        "\n",
        "### Step 3 ‚Äî Add deployment link\n",
        "Include the deployed application URL in your README.md file.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}